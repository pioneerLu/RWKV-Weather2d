{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolve embedding and recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding2D(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, patch_size):\n",
    "        super(PatchEmbedding2D, self).__init__()\n",
    "        self.projection = nn.Conv2d(\n",
    "            in_channels=input_dim,  \n",
    "            out_channels=embed_dim,\n",
    "            kernel_size=patch_size, #no overlapping\n",
    "            stride=patch_size\n",
    "        )\n",
    "        self.flatten = nn.Flatten(start_dim=2)  # Flatten spatial dimensions (H, W)\n",
    "\n",
    "    def forward(self, x): #(B, C, H, W)\n",
    "        x = self.projection(x)  # (B, embed_dim, H', W')\n",
    "        x = self.flatten(x)  # (B, embed_dim, H'*W')\n",
    "        x = x.transpose(1, 2)  # (B, H'*W', embed_dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PatchRecovery2D(nn.Module):\n",
    "    def __init__(self, output_dim, embed_dim, patch_size):\n",
    "        super(PatchRecovery2D, self).__init__()\n",
    "        self.reconstruction = nn.ConvTranspose2d(\n",
    "            in_channels=embed_dim,\n",
    "            out_channels=output_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x, spatial_shape):\n",
    "        # Reshape to (B, embed_dim, H', W') for ConvTranspose2d\n",
    "        B, N, C = x.shape\n",
    "        H, W = spatial_shape\n",
    "        x = x.transpose(1, 2).view(B, C, H, W)\n",
    "        return self.reconstruction(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RWKV-Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, math, gc, importlib\n",
    "import torch\n",
    "# torch._C._jit_set_profiling_executor(True)\n",
    "# torch._C._jit_set_profiling_mode(True)\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities import rank_zero_info, rank_zero_only\n",
    "from pytorch_lightning.strategies import DeepSpeedStrategy\n",
    "from transformers import CLIPVisionModel\n",
    "if importlib.util.find_spec('deepspeed'):\n",
    "    import deepspeed\n",
    "    from deepspeed.ops.adam import DeepSpeedCPUAdam, FusedAdam\n",
    "\n",
    "def __nop(ob):\n",
    "    return ob\n",
    "\n",
    "MyModule = nn.Module\n",
    "MyFunction = __nop\n",
    "if os.environ[\"RWKV_JIT_ON\"] == \"1\":\n",
    "    MyModule = torch.jit.ScriptModule\n",
    "    MyFunction = torch.jit.script_method\n",
    "\n",
    "\n",
    "########################################################################################################\n",
    "# CUDA Kernel\n",
    "########################################################################################################\n",
    "\n",
    "from torch.utils.cpp_extension import load\n",
    "\n",
    "HEAD_SIZE = int(os.environ[\"RWKV_HEAD_SIZE_A\"])\n",
    "wkv6_cuda = load(name=\"wkv6\", sources=[\"cuda/wkv6_op.cpp\", f\"cuda/wkv6_cuda.cu\"],\n",
    "                    verbose=True, extra_cuda_cflags=[\"-res-usage\", \"--use_fast_math\", \"-O3\", \"-Xptxas -O3\", \"--extra-device-vectorization\", f\"-D_N_={HEAD_SIZE}\", f\"-D_T_={int(os.environ['RWKV_CTXLEN'])}\"])\n",
    "    \n",
    "class WKV_6(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, B, T, C, H, r, k, v, w, u):\n",
    "        with torch.no_grad():\n",
    "            assert r.dtype == torch.bfloat16\n",
    "            assert k.dtype == torch.bfloat16\n",
    "            assert v.dtype == torch.bfloat16\n",
    "            assert w.dtype == torch.bfloat16\n",
    "            assert u.dtype == torch.bfloat16\n",
    "            assert HEAD_SIZE == C // H\n",
    "            ctx.B = B\n",
    "            ctx.T = T\n",
    "            ctx.C = C\n",
    "            ctx.H = H\n",
    "            assert r.is_contiguous()\n",
    "            assert k.is_contiguous()\n",
    "            assert v.is_contiguous()\n",
    "            assert w.is_contiguous()\n",
    "            assert u.is_contiguous()\n",
    "            ew = (-torch.exp(w.float())).contiguous()\n",
    "            ctx.save_for_backward(r, k, v, ew, u)\n",
    "            y = torch.empty((B, T, C), device=r.device, dtype=torch.bfloat16, memory_format=torch.contiguous_format)#.uniform_(-100, 100)\n",
    "            wkv6_cuda.forward(B, T, C, H, r, k, v, ew, u, y)\n",
    "            return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, gy):\n",
    "        with torch.no_grad():\n",
    "            assert gy.dtype == torch.bfloat16\n",
    "            B = ctx.B\n",
    "            T = ctx.T\n",
    "            C = ctx.C\n",
    "            H = ctx.H\n",
    "            assert gy.is_contiguous()\n",
    "            r, k, v, ew, u = ctx.saved_tensors\n",
    "            gr = torch.empty((B, T, C), device=gy.device, requires_grad=False, dtype=torch.bfloat16, memory_format=torch.contiguous_format)#.uniform_(-100, 100)\n",
    "            gk = torch.empty((B, T, C), device=gy.device, requires_grad=False, dtype=torch.bfloat16, memory_format=torch.contiguous_format)#.uniform_(-100, 100)\n",
    "            gv = torch.empty((B, T, C), device=gy.device, requires_grad=False, dtype=torch.bfloat16, memory_format=torch.contiguous_format)#.uniform_(-100, 100)\n",
    "            gw = torch.empty((B, T, C), device=gy.device, requires_grad=False, dtype=torch.bfloat16, memory_format=torch.contiguous_format)#.uniform_(-100, 100)\n",
    "            gu = torch.empty((B, C), device=gy.device, requires_grad=False, dtype=torch.bfloat16, memory_format=torch.contiguous_format)#.uniform_(-100, 100)\n",
    "            wkv6_cuda.backward(B, T, C, H, r, k, v, ew, u, gy, gr, gk, gv, gw, gu)\n",
    "            gu = torch.sum(gu, 0).view(H, C//H)\n",
    "            return (None, None, None, None, gr, gk, gv, gw, gu)\n",
    "\n",
    "def RUN_CUDA_RWKV6(B, T, C, H, r, k, v, w, u):\n",
    "    return WKV_6.apply(B, T, C, H, r, k, v, w, u)\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "class RWKV_Tmix_x060(MyModule):\n",
    "    def __init__(self, args, layer_id):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.layer_id = layer_id\n",
    "\n",
    "        self.head_size = args.head_size_a\n",
    "        self.n_head = args.dim_att // self.head_size\n",
    "        assert args.dim_att % self.n_head == 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ratio_0_to_1 = layer_id / (args.n_layer - 1)  # 0 to 1\n",
    "            ratio_1_to_almost0 = 1.0 - (layer_id / args.n_layer)  # 1 to ~0\n",
    "            ddd = torch.ones(1, 1, args.n_embd)\n",
    "            for i in range(args.n_embd):\n",
    "                ddd[0, 0, i] = i / args.n_embd\n",
    "\n",
    "            # fancy time_mix\n",
    "            self.time_maa_x = nn.Parameter(1.0 - torch.pow(ddd, ratio_1_to_almost0))\n",
    "            self.time_maa_w = nn.Parameter(1.0 - torch.pow(ddd, ratio_1_to_almost0))\n",
    "            self.time_maa_k = nn.Parameter(1.0 - torch.pow(ddd, ratio_1_to_almost0))\n",
    "            self.time_maa_v = nn.Parameter(1.0 - (torch.pow(ddd, ratio_1_to_almost0) + 0.3 * ratio_0_to_1))\n",
    "            self.time_maa_r = nn.Parameter(1.0 - torch.pow(ddd, 0.5 * ratio_1_to_almost0))\n",
    "            self.time_maa_g = nn.Parameter(1.0 - torch.pow(ddd, 0.5 * ratio_1_to_almost0))\n",
    "\n",
    "            D_MIX_LORA = 32 # generate TIME_MIX for w,k,v,r,g\n",
    "            if args.n_embd >= 4096:\n",
    "                D_MIX_LORA = 64\n",
    "            self.time_maa_w1 = nn.Parameter(torch.zeros(args.n_embd, D_MIX_LORA*5))\n",
    "            self.time_maa_w2 = nn.Parameter(torch.zeros(5, D_MIX_LORA, args.n_embd).uniform_(-0.01, 0.01))\n",
    "\n",
    "            # fancy time_decay\n",
    "            decay_speed = torch.ones(args.dim_att)\n",
    "            for n in range(args.dim_att):\n",
    "                decay_speed[n] = -6 + 5 * (n / (args.dim_att - 1)) ** (0.7 + 1.3 * ratio_0_to_1)\n",
    "            self.time_decay = nn.Parameter(decay_speed.reshape(1,1,args.dim_att))\n",
    "\n",
    "            D_DECAY_LORA = 64\n",
    "            if args.n_embd >= 4096:\n",
    "                D_DECAY_LORA = 128\n",
    "            self.time_decay_w1 = nn.Parameter(torch.zeros(args.n_embd, D_DECAY_LORA))\n",
    "            self.time_decay_w2 = nn.Parameter(torch.zeros(D_DECAY_LORA, args.dim_att).uniform_(-0.01, 0.01))\n",
    "\n",
    "            tmp = torch.zeros(args.dim_att)\n",
    "            for n in range(args.dim_att):\n",
    "                zigzag = ((n + 1) % 3 - 1) * 0.1\n",
    "                tmp[n] = ratio_0_to_1 * (1 - (n / (args.dim_att - 1))) + zigzag\n",
    "\n",
    "            self.time_faaaa = nn.Parameter(tmp.reshape(self.n_head, self.head_size))\n",
    "\n",
    "        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n",
    "        self.receptance = nn.Linear(args.n_embd, args.dim_att, bias=False)\n",
    "        self.key = nn.Linear(args.n_embd, args.dim_att, bias=False)\n",
    "\n",
    "        self.value = nn.Linear(args.n_embd, args.dim_att, bias=False)\n",
    "        self.output = nn.Linear(args.dim_att, args.n_embd, bias=False)\n",
    "        self.gate = nn.Linear(args.n_embd, args.dim_att, bias=False)\n",
    "        self.ln_x = nn.GroupNorm(self.n_head, args.dim_att, eps=(1e-5)*(args.head_size_divisor**2))\n",
    "\n",
    "    @MyFunction\n",
    "    def jit_func(self, x):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        xx = self.time_shift(x) - x\n",
    "\n",
    "        xxx = x + xx * self.time_maa_x\n",
    "        xxx = torch.tanh(xxx @ self.time_maa_w1).view(B*T, 5, -1).transpose(0, 1)\n",
    "        xxx = torch.bmm(xxx, self.time_maa_w2).view(5, B, T, -1)\n",
    "        mw, mk, mv, mr, mg = xxx.unbind(dim=0)\n",
    "\n",
    "        xw = x + xx * (self.time_maa_w + mw)\n",
    "        xk = x + xx * (self.time_maa_k + mk)\n",
    "        xv = x + xx * (self.time_maa_v + mv)\n",
    "        xr = x + xx * (self.time_maa_r + mr)\n",
    "        xg = x + xx * (self.time_maa_g + mg)\n",
    "\n",
    "        r = self.receptance(xr)\n",
    "        k = self.key(xk)\n",
    "        v = self.value(xv)\n",
    "        g = F.silu(self.gate(xg))\n",
    "\n",
    "        ww = torch.tanh(xw @ self.time_decay_w1) @ self.time_decay_w2\n",
    "        w = self.time_decay + ww\n",
    "\n",
    "        return r, k, v, g, w\n",
    "\n",
    "    @MyFunction\n",
    "    def jit_func_2(self, x, g):\n",
    "        B, T, C = x.size()\n",
    "        x = x.view(B * T, C)\n",
    "        \n",
    "        x = self.ln_x(x).view(B, T, C)\n",
    "        x = self.output(x * g)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        H = self.n_head\n",
    "\n",
    "        r, k, v, g, w = self.jit_func(x)\n",
    "        x = RUN_CUDA_RWKV6(B, T, C, H, r, k, v, w, u=self.time_faaaa)\n",
    "\n",
    "        return self.jit_func_2(x, g)\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "class RWKV_CMix_x060(MyModule):\n",
    "    def __init__(self, args, layer_id):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.layer_id = layer_id\n",
    "        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n",
    "\n",
    "        with torch.no_grad():  # fancy init of time_mix\n",
    "            ratio_1_to_almost0 = 1.0 - (layer_id / args.n_layer)  # 1 to ~0\n",
    "            ddd = torch.ones(1, 1, args.n_embd)\n",
    "            for i in range(args.n_embd):\n",
    "                ddd[0, 0, i] = i / args.n_embd\n",
    "            self.time_maa_k = nn.Parameter(1.0 - torch.pow(ddd, ratio_1_to_almost0))\n",
    "            self.time_maa_r = nn.Parameter(1.0 - torch.pow(ddd, ratio_1_to_almost0))\n",
    "\n",
    "        self.key = nn.Linear(args.n_embd, args.dim_ffn, bias=False)\n",
    "        self.receptance = nn.Linear(args.n_embd, args.n_embd, bias=False)\n",
    "        self.value = nn.Linear(args.dim_ffn, args.n_embd, bias=False)\n",
    "\n",
    "    @MyFunction\n",
    "    def forward(self, x):\n",
    "        xx = self.time_shift(x) - x\n",
    "        xk = x + xx * self.time_maa_k\n",
    "        xr = x + xx * self.time_maa_r\n",
    "\n",
    "        k = self.key(xk)\n",
    "        k = torch.relu(k) ** 2\n",
    "        kv = self.value(k)\n",
    "        return torch.sigmoid(self.receptance(xr)) * kv\n",
    "    \n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, args, layer_id):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.layer_id = layer_id\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(args.n_embd)\n",
    "        self.ln2 = nn.LayerNorm(args.n_embd)\n",
    "\n",
    "        if self.layer_id == 0:\n",
    "            self.ln0 = nn.LayerNorm(args.n_embd)\n",
    "\n",
    "        self.att = RWKV_Tmix_x060(args, layer_id)\n",
    "        self.ffn = RWKV_CMix_x060(args, layer_id)\n",
    "\n",
    "        if args.dropout > 0:\n",
    "            self.drop0 = nn.Dropout(p = args.dropout)\n",
    "            self.drop1 = nn.Dropout(p = args.dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.layer_id == 0:\n",
    "            x = self.ln0(x)\n",
    "\n",
    "        x = x + self.att(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class L2Wrap(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, loss, y):\n",
    "        ctx.save_for_backward(y)\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        y = ctx.saved_tensors[0]\n",
    "        # to encourage the logits to be close to 0\n",
    "        factor = 1e-4 / (y.shape[0] * y.shape[1])\n",
    "        maxx, ids = torch.max(y, -1, keepdim=True)\n",
    "        gy = torch.zeros_like(y)\n",
    "        gy.scatter_(-1, ids, maxx * factor)\n",
    "        return (grad_output, gy)\n",
    "    \n",
    "\n",
    "\n",
    "class RWKV(pl.LightningModule):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.emb = nn.Embedding(args.vocab_size, args.n_embd)\n",
    "        self.blocks = nn.ModuleList([Block(args, i) for i in range(args.n_layer)])\n",
    "        self.ln_out = nn.LayerNorm(args.n_embd)\n",
    "        self.head = nn.Linear(args.n_embd, args.vocab_size, bias=False)\n",
    "\n",
    "        if args.dropout > 0:\n",
    "            self.drop0 = nn.Dropout(p = args.dropout)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        trainable_params = [p for p in self.parameters() if p.requires_grad]\n",
    "        optim_groups = [{\"params\": trainable_params, \"weight_decay\": self.args.weight_decay}]\n",
    "        if self.deepspeed_offload:\n",
    "            return DeepSpeedCPUAdam(optim_groups, lr=self.args.lr_init, betas=self.args.betas, eps=self.args.adam_eps, bias_correction=True, adamw_mode=True, amsgrad=False)\n",
    "        return FusedAdam(optim_groups, lr=self.args.lr_init, betas=self.args.betas, eps=self.args.adam_eps, bias_correction=True, adam_w_mode=True, amsgrad=False)\n",
    "\n",
    "    @property\n",
    "    def deepspeed_offload(self) -> bool:\n",
    "        strategy = self.trainer.strategy\n",
    "        if isinstance(strategy, DeepSpeedStrategy):\n",
    "            cfg = strategy.config[\"zero_optimization\"]\n",
    "            return cfg.get(\"offload_optimizer\") or cfg.get(\"offload_param\")\n",
    "        return False\n",
    "\n",
    "    def forward(self, x):\n",
    "        args = self.args\n",
    "        if args.dropout > 0:\n",
    "            x = self.drop0(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            if args.grad_cp == 1:\n",
    "                x = deepspeed.checkpointing.checkpoint(block, x)\n",
    "            else:\n",
    "                x = block(x)\n",
    "\n",
    "        x = self.ln_out(x)\n",
    "\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        idx, targets = batch\n",
    "        logits = self(idx)\n",
    "        loss = F.mse_loss(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return L2Wrap.apply(loss, logits)\n",
    "\n",
    "    def training_step_end(self, batch_parts):\n",
    "        if pl.__version__[0]!='2':\n",
    "            all = self.all_gather(batch_parts)\n",
    "            if self.trainer.is_global_zero:\n",
    "                self.trainer.my_loss_all = all\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    pass\n",
    "class Encoder(nn.Module):\n",
    "    pass\n",
    "\n",
    "class RWKV_block(pl.LightningModule):\n",
    "    def __init__(self,args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.rwkv = RWKV(args)\n",
    "        if args.load_model:\n",
    "            self.load_rwkv_from_pretrained(args.load_model)\n",
    "\n",
    "    def load_rwkv_from_pretrained(self, path):\n",
    "        self.rwkv.load_state_dict(torch.load(path, map_location=\"cpu\"))\n",
    "    \n",
    "    def forward(self,x):\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "class RWKV_weather(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownSample(nn.Module):\n",
    "    pass\n",
    "\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    pass\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
