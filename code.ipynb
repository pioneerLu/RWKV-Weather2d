{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rwkv/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 1261\n",
      "Number of train samples: 1261\n",
      "Number of train samples: 1261\n"
     ]
    }
   ],
   "source": [
    "from dataset_test import RWKVWeatherDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = RWKVWeatherDataset('/home/rwkv/RWKV-TS/WeatherBench/era5_data/ERA5_merged(2010-2018).nc',flag='train',split=0.8)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0,drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolve embedding and recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PatchEmbedding3D(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, patch_size):\n",
    "        super(PatchEmbedding3D, self).__init__()\n",
    "        self.projection = nn.Conv3d(\n",
    "            in_channels=input_dim,\n",
    "            out_channels=embed_dim,\n",
    "            kernel_size=patch_size,  # no overlapping\n",
    "            stride=patch_size,\n",
    "            # padding=(15, 0, 0)\n",
    "        )\n",
    "        self.flatten = nn.Flatten(start_dim=2)  # Flatten spatial and time dimensions (T, H, W)\n",
    "\n",
    "    def forward(self, x):  # (B, C, T, H, W)\n",
    "        x = self.projection(x)  # (B, embed_dim, T', H', W')\n",
    "        print(x.shape)\n",
    "        x = self.flatten(x)  # (B, embed_dim, T'*H'*W')\n",
    "        x = x.transpose(1, 2)  # (B, T'*H'*W', embed_dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PatchRecovery3D(nn.Module):\n",
    "    def __init__(self, output_dim, embed_dim, patch_size):\n",
    "        super(PatchRecovery3D, self).__init__()\n",
    "        self.reconstruction = nn.ConvTranspose3d(\n",
    "            in_channels=embed_dim,\n",
    "            out_channels=output_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size,\n",
    "            # padding=patch_size\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x, temporal_shape, spatial_shape):\n",
    "        # Reshape to (B, embed_dim, T', H', W') for ConvTranspose3d\n",
    "        B, N, C = x.shape\n",
    "        T, H, W = temporal_shape, *spatial_shape\n",
    "        x = x.transpose(1, 2).view(B, C, T, H, W)\n",
    "        return self.reconstruction(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 1261\n",
      "Number of train samples: 1261\n",
      "input shape: torch.Size([4, 5, 10, 120, 120])\n",
      "torch.Size([4, 512, 5, 30, 30])\n",
      "embeddings shape: torch.Size([4, 4500, 512])\n",
      "recovered shape: torch.Size([4, 5, 10, 120, 120])\n",
      "target shape: torch.Size([4, 5, 10, 120, 120])\n",
      "loss: tensor(8.3411e+09, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     loss \u001b[38;5;241m=\u001b[39m (weights \u001b[38;5;241m*\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(pred_diffs, target_diffs, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     28\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m patch_embedding(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/RWKV-TS/WeatherBench/dataset_test.py:75\u001b[0m, in \u001b[0;36mRWKVWeatherDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     67\u001b[0m     target\u001b[38;5;241m.\u001b[39mappend(target_data)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# sample = {\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m#     'input': np.concatenate(input_points, axis=1),\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m#     'target': np.concatenate(target, axis=1),\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# }\u001b[39;00m\n\u001b[1;32m     74\u001b[0m sample \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_points\u001b[49m\u001b[43m)\u001b[49m[idx],\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39marray(target)[idx],\n\u001b[1;32m     77\u001b[0m }\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sample\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RWKV-Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/rwkv/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module wkv6, skipping build step...\n",
      "Loading extension module wkv6...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, math, gc, importlib\n",
    "import torch\n",
    "# torch._C._jit_set_profiling_executor(True)\n",
    "# torch._C._jit_set_profiling_mode(True)\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities import rank_zero_info, rank_zero_only\n",
    "from pytorch_lightning.strategies import DeepSpeedStrategy\n",
    "from transformers import CLIPVisionModel\n",
    "if importlib.util.find_spec('deepspeed'):\n",
    "    import deepspeed\n",
    "    from deepspeed.ops.adam import DeepSpeedCPUAdam, FusedAdam\n",
    "\n",
    "def __nop(ob):\n",
    "    return ob\n",
    "\n",
    "MyModule = nn.Module\n",
    "MyFunction = __nop\n",
    "# if os.environ[\"RWKV_JIT_ON\"] == \"1\":\n",
    "\n",
    "MyModule = torch.jit.ScriptModule\n",
    "MyFunction = torch.jit.script_method\n",
    "\n",
    "\n",
    "########################################################################################################\n",
    "# CUDA Kernel\n",
    "########################################################################################################\n",
    "\n",
    "from torch.utils.cpp_extension import load\n",
    "\n",
    "HEAD_SIZE = 64\n",
    "wkv6_cuda = load(name=\"wkv6\", sources=[\"cuda/wkv6_op.cpp\", f\"cuda/wkv6_cuda.cu\"],\n",
    "                    verbose=True, extra_cuda_cflags=[\"-res-usage\", \"--use_fast_math\", \"-O3\", \"-Xptxas -O3\", \"--extra-device-vectorization\", f\"-D_N_={HEAD_SIZE}\", f\"-D_T_={int(10)}\"])\n",
    "    \n",
    "class WKV_6(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, B, T, C, H, r, k, v, w, u):\n",
    "        with torch.no_grad():\n",
    "            assert r.dtype == torch.bfloat16\n",
    "            assert k.dtype == torch.bfloat16\n",
    "            assert v.dtype == torch.bfloat16\n",
    "            assert w.dtype == torch.bfloat16\n",
    "            assert u.dtype == torch.bfloat16\n",
    "            assert HEAD_SIZE == C // H\n",
    "            ctx.B = B\n",
    "            ctx.T = T\n",
    "            ctx.C = C\n",
    "            ctx.H = H\n",
    "            assert r.is_contiguous()\n",
    "            assert k.is_contiguous()\n",
    "            assert v.is_contiguous()\n",
    "            assert w.is_contiguous()\n",
    "            assert u.is_contiguous()\n",
    "            ew = (-torch.exp(w.float())).contiguous()\n",
    "            ctx.save_for_backward(r, k, v, ew, u)\n",
    "            y = torch.empty((B, T, C), device=r.device, dtype=torch.bfloat16, memory_format=torch.contiguous_format)#.uniform_(-100, 100)\n",
    "            wkv6_cuda.forward(B, T, C, H, r, k, v, ew, u, y)\n",
    "            return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, gy):\n",
    "        with torch.no_grad():\n",
    "            assert gy.dtype == torch.bfloat16\n",
    "            B = ctx.B\n",
    "            T = ctx.T\n",
    "            C = ctx.C\n",
    "            H = ctx.H\n",
    "            assert gy.is_contiguous()\n",
    "            r, k, v, ew, u = ctx.saved_tensors\n",
    "            gr = torch.empty((B, T, C), device=gy.device, requires_grad=False, dtype=torch.bfloat16, memory_format=torch.contiguous_format)#.uniform_(-100, 100)\n",
    "            gk = torch.empty((B, T, C), device=gy.device, requires_grad=False, dtype=torch.bfloat16, memory_format=torch.contiguous_format)#.uniform_(-100, 100)\n",
    "            gv = torch.empty((B, T, C), device=gy.device, requires_grad=False, dtype=torch.bfloat16, memory_format=torch.contiguous_format)#.uniform_(-100, 100)\n",
    "            gw = torch.empty((B, T, C), device=gy.device, requires_grad=False, dtype=torch.bfloat16, memory_format=torch.contiguous_format)#.uniform_(-100, 100)\n",
    "            gu = torch.empty((B, C), device=gy.device, requires_grad=False, dtype=torch.bfloat16, memory_format=torch.contiguous_format)#.uniform_(-100, 100)\n",
    "            wkv6_cuda.backward(B, T, C, H, r, k, v, ew, u, gy, gr, gk, gv, gw, gu)\n",
    "            gu = torch.sum(gu, 0).view(H, C//H)\n",
    "            return (None, None, None, None, gr, gk, gv, gw, gu)\n",
    "\n",
    "def RUN_CUDA_RWKV6(B, T, C, H, r, k, v, w, u):\n",
    "    return WKV_6.apply(B, T, C, H, r, k, v, w, u)\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "class RWKV_Tmix_x060(MyModule):\n",
    "    def __init__(self, args, layer_id):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.layer_id = layer_id\n",
    "\n",
    "        self.head_size = 64\n",
    "        self.n_head = args.dim_att // self.head_size\n",
    "        assert args.dim_att % self.n_head == 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # print(args.n_layer)\n",
    "            ratio_0_to_1 = layer_id / (args.n_layer - 1)  # 0 to 1\n",
    "            ratio_1_to_almost0 = 1.0 - (layer_id / args.n_layer)  # 1 to ~0\n",
    "            ddd = torch.ones(1, 1, args.n_embd)\n",
    "            for i in range(args.n_embd):\n",
    "                ddd[0, 0, i] = i / args.n_embd\n",
    "\n",
    "            # fancy time_mix\n",
    "            self.time_maa_x = nn.Parameter(1.0 - torch.pow(ddd, ratio_1_to_almost0))\n",
    "            self.time_maa_w = nn.Parameter(1.0 - torch.pow(ddd, ratio_1_to_almost0))\n",
    "            self.time_maa_k = nn.Parameter(1.0 - torch.pow(ddd, ratio_1_to_almost0))\n",
    "            self.time_maa_v = nn.Parameter(1.0 - (torch.pow(ddd, ratio_1_to_almost0) + 0.3 * ratio_0_to_1))\n",
    "            self.time_maa_r = nn.Parameter(1.0 - torch.pow(ddd, 0.5 * ratio_1_to_almost0))\n",
    "            self.time_maa_g = nn.Parameter(1.0 - torch.pow(ddd, 0.5 * ratio_1_to_almost0))\n",
    "\n",
    "            D_MIX_LORA = 32 # generate TIME_MIX for w,k,v,r,g\n",
    "            if args.n_embd >= 4096:\n",
    "                D_MIX_LORA = 64\n",
    "            self.time_maa_w1 = nn.Parameter(torch.zeros(args.n_embd, D_MIX_LORA*5))\n",
    "            self.time_maa_w2 = nn.Parameter(torch.zeros(5, D_MIX_LORA, args.n_embd).uniform_(-0.01, 0.01))\n",
    "\n",
    "            # fancy time_decay\n",
    "            decay_speed = torch.ones(args.dim_att)\n",
    "            for n in range(args.dim_att):\n",
    "                decay_speed[n] = -6 + 5 * (n / (args.dim_att - 1)) ** (0.7 + 1.3 * ratio_0_to_1)\n",
    "            self.time_decay = nn.Parameter(decay_speed.reshape(1,1,args.dim_att))\n",
    "\n",
    "            D_DECAY_LORA = 64\n",
    "            if args.n_embd >= 4096:\n",
    "                D_DECAY_LORA = 128\n",
    "            self.time_decay_w1 = nn.Parameter(torch.zeros(args.n_embd, D_DECAY_LORA))\n",
    "            self.time_decay_w2 = nn.Parameter(torch.zeros(D_DECAY_LORA, args.dim_att).uniform_(-0.01, 0.01))\n",
    "\n",
    "            tmp = torch.zeros(args.dim_att)\n",
    "            for n in range(args.dim_att):\n",
    "                zigzag = ((n + 1) % 3 - 1) * 0.1\n",
    "                tmp[n] = ratio_0_to_1 * (1 - (n / (args.dim_att - 1))) + zigzag\n",
    "\n",
    "            self.time_faaaa = nn.Parameter(tmp.reshape(self.n_head, self.head_size))\n",
    "\n",
    "        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n",
    "        self.receptance = nn.Linear(args.n_embd, args.dim_att, bias=False)\n",
    "        self.key = nn.Linear(args.n_embd, args.dim_att, bias=False)\n",
    "\n",
    "        self.value = nn.Linear(args.n_embd, args.dim_att, bias=False)\n",
    "        self.output = nn.Linear(args.dim_att, args.n_embd, bias=False)\n",
    "        self.gate = nn.Linear(args.n_embd, args.dim_att, bias=False)\n",
    "        self.ln_x = nn.GroupNorm(self.n_head, args.dim_att, eps=(1e-5)*(args.head_size_divisor**2))\n",
    "\n",
    "    @MyFunction\n",
    "    def jit_func(self, x):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        xx = self.time_shift(x) - x\n",
    "\n",
    "        xxx = x + xx * self.time_maa_x\n",
    "        xxx = torch.tanh(xxx @ self.time_maa_w1).view(B*T, 5, -1).transpose(0, 1)\n",
    "        xxx = torch.bmm(xxx, self.time_maa_w2).view(5, B, T, -1)\n",
    "        mw, mk, mv, mr, mg = xxx.unbind(dim=0)\n",
    "\n",
    "        xw = x + xx * (self.time_maa_w + mw)\n",
    "        xk = x + xx * (self.time_maa_k + mk)\n",
    "        xv = x + xx * (self.time_maa_v + mv)\n",
    "        xr = x + xx * (self.time_maa_r + mr)\n",
    "        xg = x + xx * (self.time_maa_g + mg)\n",
    "\n",
    "        r = self.receptance(xr)\n",
    "        k = self.key(xk)\n",
    "        v = self.value(xv)\n",
    "        g = F.silu(self.gate(xg))\n",
    "\n",
    "        ww = torch.tanh(xw @ self.time_decay_w1) @ self.time_decay_w2\n",
    "        w = self.time_decay + ww\n",
    "\n",
    "        return r, k, v, g, w\n",
    "\n",
    "    @MyFunction\n",
    "    def jit_func_2(self, x, g):\n",
    "        B, T, C = x.size()\n",
    "        x = x.view(B * T, C)\n",
    "        \n",
    "        x = self.ln_x(x).view(B, T, C)\n",
    "        x = self.output(x * g)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        H = self.n_head\n",
    "\n",
    "        r, k, v, g, w = self.jit_func(x)\n",
    "        x = RUN_CUDA_RWKV6(B, T, C, H, r, k, v, w, u=self.time_faaaa)\n",
    "\n",
    "        return self.jit_func_2(x, g)\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "class RWKV_CMix_x060(MyModule):\n",
    "    def __init__(self, args, layer_id):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.layer_id = layer_id\n",
    "        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n",
    "\n",
    "        with torch.no_grad():  # fancy init of time_mix\n",
    "            ratio_1_to_almost0 = 1.0 - (layer_id / args.n_layer)  # 1 to ~0\n",
    "            ddd = torch.ones(1, 1, args.n_embd)\n",
    "            for i in range(args.n_embd):\n",
    "                ddd[0, 0, i] = i / args.n_embd\n",
    "            self.time_maa_k = nn.Parameter(1.0 - torch.pow(ddd, ratio_1_to_almost0))\n",
    "            self.time_maa_r = nn.Parameter(1.0 - torch.pow(ddd, ratio_1_to_almost0))\n",
    "\n",
    "        self.key = nn.Linear(args.n_embd, args.dim_ffn, bias=False)\n",
    "        self.receptance = nn.Linear(args.n_embd, args.n_embd, bias=False)\n",
    "        self.value = nn.Linear(args.dim_ffn, args.n_embd, bias=False)\n",
    "\n",
    "    @MyFunction\n",
    "    def forward(self, x):\n",
    "        xx = self.time_shift(x) - x\n",
    "        xk = x + xx * self.time_maa_k\n",
    "        xr = x + xx * self.time_maa_r\n",
    "\n",
    "        k = self.key(xk)\n",
    "        k = torch.relu(k) ** 2\n",
    "        kv = self.value(k)\n",
    "        return torch.sigmoid(self.receptance(xr)) * kv\n",
    "    \n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, args, layer_id):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.layer_id = layer_id\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(args.n_embd)\n",
    "        self.ln2 = nn.LayerNorm(args.n_embd)\n",
    "\n",
    "        if self.layer_id == 0:\n",
    "            self.ln0 = nn.LayerNorm(args.n_embd)\n",
    "\n",
    "        self.att = RWKV_Tmix_x060(args, layer_id)\n",
    "        self.ffn = RWKV_CMix_x060(args, layer_id)\n",
    "\n",
    "        if args.dropout > 0:\n",
    "            self.drop0 = nn.Dropout(p = args.dropout)\n",
    "            self.drop1 = nn.Dropout(p = args.dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.layer_id == 0:\n",
    "            x = self.ln0(x)\n",
    "\n",
    "        x = x + self.att(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class L2Wrap(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, loss, y):\n",
    "        ctx.save_for_backward(y)\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        y = ctx.saved_tensors[0]\n",
    "        # to encourage the logits to be close to 0\n",
    "        factor = 1e-4 / (y.shape[0] * y.shape[1])\n",
    "        maxx, ids = torch.max(y, -1, keepdim=True)\n",
    "        gy = torch.zeros_like(y)\n",
    "        gy.scatter_(-1, ids, maxx * factor)\n",
    "        return (grad_output, gy)\n",
    "    \n",
    "\n",
    "\n",
    "class RWKV_Layer(pl.LightningModule):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.emb = nn.Embedding(args.vocab_size, args.n_embd)\n",
    "        self.blocks = nn.ModuleList([Block(args, i) for i in range(args.n_layer)])\n",
    "        # print(args.n_layer)\n",
    "        self.ln_out = nn.LayerNorm(args.n_embd)\n",
    "        self.head = nn.Linear(args.n_embd, args.vocab_size, bias=False)\n",
    "\n",
    "        if args.dropout > 0:\n",
    "            self.drop0 = nn.Dropout(p = args.dropout)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        trainable_params = [p for p in self.parameters() if p.requires_grad]\n",
    "        optim_groups = [{\"params\": trainable_params, \"weight_decay\": self.args.weight_decay}]\n",
    "        if self.deepspeed_offload:\n",
    "            return DeepSpeedCPUAdam(optim_groups, lr=self.args.lr_init, betas=self.args.betas, eps=self.args.adam_eps, bias_correction=True, adamw_mode=True, amsgrad=False)\n",
    "        return FusedAdam(optim_groups, lr=self.args.lr_init, betas=self.args.betas, eps=self.args.adam_eps, bias_correction=True, adam_w_mode=True, amsgrad=False)\n",
    "\n",
    "    @property\n",
    "    def deepspeed_offload(self) -> bool:\n",
    "        strategy = self.trainer.strategy\n",
    "        if isinstance(strategy, DeepSpeedStrategy):\n",
    "            cfg = strategy.config[\"zero_optimization\"]\n",
    "            return cfg.get(\"offload_optimizer\") or cfg.get(\"offload_param\")\n",
    "        return False\n",
    "\n",
    "    def forward(self, x):\n",
    "        args = self.args\n",
    "        if args.dropout > 0:\n",
    "            x = self.drop0(x)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            if args.grad_cp == 1:\n",
    "                x = deepspeed.checkpointing.checkpoint(block, x)\n",
    "            else:\n",
    "                x = block(x)\n",
    "\n",
    "        x = self.ln_out(x)\n",
    "\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        idx, targets = batch\n",
    "        logits = self(idx)\n",
    "        loss = F.mse_loss(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return L2Wrap.apply(loss, logits)\n",
    "\n",
    "    def training_step_end(self, batch_parts):\n",
    "        if pl.__version__[0]!='2':\n",
    "            all = self.all_gather(batch_parts)\n",
    "            if self.trainer.is_global_zero:\n",
    "                self.trainer.my_loss_all = all\n",
    "\n",
    "##Not sure below\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        \"\"\"\n",
    "        3D Down-sampling operation\n",
    "        Args:\n",
    "            dim (int): Input channel dimension\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv_down = nn.Conv3d(4 * dim, 2 * dim, kernel_size=1, stride=1, padding=0) \n",
    "        self.norm = nn.LayerNorm(4 * dim) \n",
    "\n",
    "    def forward(self, x, T, H, W):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor [B, T*H*W, dim]\n",
    "            T (int): Temporal dimension (time steps)\n",
    "            H (int): Height\n",
    "            W (int): Width\n",
    "        \"\"\"\n",
    "        # x: [B, T*H*W, dim] -> [B, T, H, W, dim]\n",
    "        print(x.shape)\n",
    "        x = x.reshape(x.shape[0], T, H//2, W//2, -1)\n",
    "        x = self.norm(x)\n",
    "        # Reshape to [B, dim, T, H, W] for Conv3d\n",
    "        x = x.permute(0, 4, 1, 2, 3)\n",
    "        x = self.conv_down(x)\n",
    "        x = x.reshape(x.shape[0],-1,x.shape[1])\n",
    "        return x\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv_up = nn.Conv3d(2 * dim, 4 * dim, kernel_size=1, stride=1, padding=0)  \n",
    "        self.norm = nn.LayerNorm(4 * dim)  \n",
    "\n",
    "    def forward(self, x, T, H, W):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor [B, -1, dim]\n",
    "            T (int): Temporal dimension (time steps)\n",
    "            H (int): Height\n",
    "            W (int): Width\n",
    "        \"\"\"\n",
    "        x = x.reshape(x.shape[0], -1, T, H//2, W//2)\n",
    "\n",
    "        # Apply 3D convolution\n",
    "        x = self.conv_up(x)\n",
    "\n",
    "        # Reshape to [B, T, H, W, 4*dim]\n",
    "        x = x.permute(0, 2, 3, 4, 1)\n",
    "\n",
    "        # Apply normalization\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Reshape back to [B, T*H*W, dim]\n",
    "        x = x.reshape(x.shape[0], T * H * W, -1)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    pass\n",
    "class Encoder(nn.Module):\n",
    "    pass\n",
    "\n",
    "\n",
    "class RWKV_Weather(pl.LightningModule):\n",
    "    def __init__(self,args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.layer1 = RWKV_Layer(args)\n",
    "        self.layer2 = RWKV_Layer(args)\n",
    "        self.layer3 = RWKV_Layer(args)\n",
    "        self.layer4 = RWKV_Layer(args)\n",
    "        \n",
    "        self.downsample = DownSample(args.n_embd)\n",
    "        self.upsample = UpSample(args.n_embd)\n",
    "        self.patch_embed = PatchEmbedding3D(5,args.n_embd,(2,4,4))###\n",
    "        self.patch_reovery = PatchRecovery3D(5, args.n_embd,(2,4,4))###\n",
    "        if args.load_model:\n",
    "            self.load_rwkv_from_pretrained(args.load_model)\n",
    "\n",
    "    def load_rwkv_from_pretrained(self, path):\n",
    "        self.rwkv.load_state_dict(torch.load(path, map_location=\"cpu\"))\n",
    "    \n",
    "    def forward(self,samples):\n",
    "        x,y= samples['input'],samples['target']\n",
    "        \n",
    "        x = self.patch_embed(x)\n",
    "        x = self.layer1(x)\n",
    "        print(x.shape)\n",
    "        skip = x\n",
    "\n",
    "        x = self.downsample(x,10,(30,30))\n",
    "        x = self.layer2(x)\n",
    "        x= self.layer3(x)\n",
    "        x = self.upsample(x,10,15,15)\n",
    "        x = self.layer4(x)\n",
    "        x = x + skip\n",
    "        x = self.patch_reovery(x,5,30,30)\n",
    "        return x,y\n",
    "    \n",
    "    def weighted_temporal_consistency_loss(predict_frames, target_frames, weights):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predict_frames [B, C, T, H, W]\n",
    "            target_frames  [B, C, T, H, W]\n",
    "            weights  [T-1]\n",
    "        \"\"\"\n",
    "\n",
    "        assert predict_frames.shape == target_frames.shape\n",
    "        num_frames = predict_frames.shape[2]\n",
    "\n",
    "        assert len(weights) == num_frames - 1\n",
    "        \n",
    "\n",
    "        pred_diffs = predict_frames[:, :, 1:] - predict_frames[:, :, :-1]  # [B, C, T-1, H, W]\n",
    "        target_diffs = target_frames[:, :, 1:] - target_frames[:, :, :-1]  # [B, C, T-1, H, W]\n",
    "        weights = torch.as_tensor(weights, dtype=torch.float32, device=predict_frames.device).view(1, 1, -1, 1, 1)  # [1, 1, T-1, 1, 1]\n",
    "        \n",
    "        loss = (weights * F.mse_loss(pred_diffs, target_diffs, reduction='none')).mean()\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs, targets = self(batch)  \n",
    "\n",
    "        loss1 = F.mse_loss(outputs, targets)\n",
    "        weights = torch.ones(len(outputs) - 1, device=outputs.device)  ###\n",
    "        loss2 = self.weighted_temporal_consistency_loss(outputs, targets, weights)###\n",
    "        loss = loss1 + loss2\n",
    "\n",
    "        self.log(\"batch_loss\", loss, on_step=True, on_epoch=False, prog_bar=True) \n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        all_losses = [x['loss'] for x in outputs if 'loss' in x]\n",
    "        train_loss = sum(all_losses) / len(all_losses)\n",
    "        self.log(\"train_loss\", train_loss, sync_dist=True)\n",
    "        my_lr = self.trainer.optimizers[0].param_groups[0][\"lr\"]\n",
    "        self.log(\"my_lr\", my_lr, sync_dist=True)\n",
    "\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs, targets = self(batch)\n",
    "        val_loss = F.mse_loss(outputs, targets)\n",
    "        self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return {\"val_loss\": val_loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        val_losses = [x['val_loss'] for x in outputs if 'val_loss' in x]\n",
    "        if val_losses:\n",
    "            avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "            self.log(\"epoch_val_loss\", avg_val_loss, sync_dist=True, prog_bar=True)\n",
    "\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, input) -> list[int]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(args.grad_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 1261\n",
      "Number of train samples: 1261\n",
      "input shape: torch.Size([4, 5, 10, 120, 120])\n",
      "torch.Size([4, 512, 5, 30, 30])\n",
      "embeddings shape: torch.Size([4, 4500, 512])\n",
      "torch.Size([4, 4500, 512])\n",
      "down shape: torch.Size([4, 1125, 1024])\n",
      "up shape: torch.Size([4, 4500, 512])\n",
      "recovered shape: torch.Size([4, 5, 10, 120, 120])\n",
      "target shape: torch.Size([4, 5, 10, 120, 120])\n",
      "input shape: torch.Size([4, 5, 10, 120, 120])\n",
      "torch.Size([4, 512, 5, 30, 30])\n",
      "embeddings shape: torch.Size([4, 4500, 512])\n",
      "torch.Size([4, 4500, 512])\n",
      "down shape: torch.Size([4, 1125, 1024])\n",
      "up shape: torch.Size([4, 4500, 512])\n",
      "recovered shape: torch.Size([4, 5, 10, 120, 120])\n",
      "target shape: torch.Size([4, 5, 10, 120, 120])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     loss \u001b[38;5;241m=\u001b[39m (weights \u001b[38;5;241m*\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(pred_diffs, target_diffs, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     29\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m patch_embedding(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/RWKV-TS/WeatherBench/dataset_test.py:76\u001b[0m, in \u001b[0;36mRWKVWeatherDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     67\u001b[0m     target\u001b[38;5;241m.\u001b[39mappend(target_data)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# sample = {\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m#     'input': np.concatenate(input_points, axis=1),\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m#     'target': np.concatenate(target, axis=1),\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# }\u001b[39;00m\n\u001b[1;32m     74\u001b[0m sample \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39marray(input_points)[idx],\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m[idx],\n\u001b[1;32m     77\u001b[0m }\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sample\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "patch_embedding = PatchEmbedding3D(5, 512, (2,4,4))\n",
    "patch_recovery = PatchRecovery3D(5, 512, (2,4,4)) \n",
    "down = DownSample(512)\n",
    "up = UpSample(512)\n",
    "\n",
    "def weighted_temporal_consistency_loss(predict_frames, target_frames, weights):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        predict_frames [B, C, T, H, W]\n",
    "        target_frames  [B, C, T, H, W]\n",
    "        weights  [T-1]\n",
    "    \"\"\"\n",
    "\n",
    "    assert predict_frames.shape == target_frames.shape\n",
    "    num_frames = predict_frames.shape[2]\n",
    "\n",
    "    assert len(weights) == num_frames - 1\n",
    "    \n",
    "\n",
    "    pred_diffs = predict_frames[:, :, 1:] - predict_frames[:, :, :-1]  # [B, C, T-1, H, W]\n",
    "    target_diffs = target_frames[:, :, 1:] - target_frames[:, :, :-1]  # [B, C, T-1, H, W]\n",
    "    weights = torch.as_tensor(weights, dtype=torch.float32, device=predict_frames.device).view(1, 1, -1, 1, 1)  # [1, 1, T-1, 1, 1]\n",
    "    \n",
    "    loss = (weights * F.mse_loss(pred_diffs, target_diffs, reduction='none')).mean()\n",
    "    return loss\n",
    "\n",
    "for data in dataloader:\n",
    "    print(\"input shape:\", data['input'].shape)\n",
    "    embeddings = patch_embedding(data['input'])\n",
    "    print(\"embeddings shape:\", embeddings.shape)\n",
    "    down_ = down(embeddings,5,30,30)\n",
    "    print(\"down shape:\", down_.shape)\n",
    "    up_ = up(down_,5,30,30)\n",
    "    print(\"up shape:\", up_.shape)\n",
    "    up_ = up_+ embeddings\n",
    "    recover = patch_recovery(up_,5,(30,30))\n",
    "    print(\"recovered shape:\", recover.shape)\n",
    "    print(\"target shape:\", data['target'].shape)\n",
    "\n",
    "\n",
    "# for data in dataloader:\n",
    "#     print(\"input shape:\", data['input'].shape)\n",
    "#     embeddings = patch_embedding(data['input'])\n",
    "#     print(\"embeddings shape:\", embeddings.shape)\n",
    "#     recover = patch_recovery(embeddings,5,(30,30))\n",
    "#     print(\"recovered shape:\", recover.shape)\n",
    "#     print(\"target shape:\", data['target'].shape)\n",
    "#     loss1 = F.mse_loss(recover.view(-1), data['target'].view(-1))\n",
    "#     weights = torch.ones(recover.shape[2] - 1)\n",
    "#     loss2 = weighted_temporal_consistency_loss(recover, data['target'], weights)\n",
    "#     loss = loss1 + loss2\n",
    "#     print(\"loss:\", loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model =RWKV_Weather(args)\n",
    "print(args.n_layer)\n",
    "\n",
    "trainer = Trainer(precision=args.precision,devices=1,num_nodes=1)\n",
    "train = Trainer.from_argparse_args(\n",
    "            args,\n",
    "            precision=args.precision,\n",
    "            devices=1,\n",
    "            num_nodes=1)\n",
    "\n",
    "trainer.fit(model,dataloader)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
