{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm.layers import DropPath\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 721, 1440) (1, 5, 13, 721, 1440)\n"
     ]
    }
   ],
   "source": [
    "surface = '/home/rwkv/RWKV-TS/WeatherBench/input_surface.npy'\n",
    "upper = '/home/rwkv/RWKV-TS/WeatherBench/input_upper.npy'\n",
    "\n",
    "import numpy as np\n",
    "surface = np.load(surface)\n",
    "upper = np.load(upper)\n",
    "# add time dimension\n",
    "surface = np.expand_dims(surface, axis=0)\n",
    "upper = np.expand_dims(upper, axis=0)\n",
    "print(surface.shape, upper.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128) (128, 128) (128, 128)\n"
     ]
    }
   ],
   "source": [
    "land_masks = np.load('/home/rwkv/RWKV-TS/WeatherBench/constant_masks/land_mask.npy')[180:308,440:568]\n",
    "soil_types = np.load('/home/rwkv/RWKV-TS/WeatherBench/constant_masks/soil_type.npy')[180:308,440:568]\n",
    "topography = np.load('/home/rwkv/RWKV-TS/WeatherBench/constant_masks/topography.npy')[180:308,440:568]\n",
    "\n",
    "print(land_masks.shape, soil_types.shape, topography.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our finall data shape will be (B,T,5,13,128,128) and (B,T,4,128,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##self defined \n",
    "# def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "#     if drop_prob == 0. or not training:\n",
    "#         return x\n",
    "#     keep_prob = 1 - drop_prob\n",
    "#     shape = (x.shape[0],) + (1,) * (x.ndim - 1)  ##(B,1,1,1,1...)\n",
    "#     random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "#     # print(random_tensor)\n",
    "#     random_tensor.floor_()  \n",
    "#     # print(random_tensor)\n",
    "#     output = x.div(keep_prob) * random_tensor  # maintain E\n",
    "#     return output\n",
    "\n",
    "\n",
    "# class DropPath(nn.Module):\n",
    "#     def __init__(self, drop_prob=None):\n",
    "#         super(DropPath, self).__init__()\n",
    "#         self.drop_prob = drop_prob\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "\n",
    "def Inference(input, input_surface, forecast_range):\n",
    "  '''Inference code, describing the algorithm of inference using models with different lead times. \n",
    "  PanguModel24, PanguModel6, PanguModel3 and PanguModel1 share the same training algorithm but differ in lead times.\n",
    "  Args:\n",
    "    input: input tensor, need to be normalized to N(0, 1) in practice\n",
    "    input_surface: target tensor, need to be normalized to N(0, 1) in practice\n",
    "    forecast_range: iteration numbers when roll out the forecast model\n",
    "  '''\n",
    "\n",
    "  # Load 4 pre-trained models with different lead times\n",
    "  PanguModel24 = LoadModel(ModelPath24)\n",
    "  PanguModel6 = LoadModel(ModelPath6)\n",
    "  PanguModel3 = LoadModel(ModelPath3)\n",
    "  PanguModel1 = LoadModel(ModelPath1)\n",
    "\n",
    "  # Load mean and std of the weather data\n",
    "  weather_mean, weather_std, weather_surface_mean, weather_surface_std = LoadStatic()\n",
    "\n",
    "  # Store initial input for different models\n",
    "  input_24, input_surface_24 = input, input_surface\n",
    "  input_6, input_surface_6 = input, input_surface\n",
    "  input_3, input_surface_3 = input, input_surface\n",
    "\n",
    "  # Using a list to store output\n",
    "  output_list = []\n",
    "\n",
    "  # Note: the following code is implemented for fast inference of [1,forecast_range]-hour forecasts -- if only one lead time is requested, the inference can be much faster.\n",
    "  for i in range(forecast_range):\n",
    "    # switch to the 24-hour model if the forecast time is 24 hours, 48 hours, ..., 24*N hours\n",
    "    if (i+1) % 24 == 0:\n",
    "      # Switch the input back to the stored input\n",
    "      input, input_surface = input_24, input_surface_24\n",
    "\n",
    "      # Call the model pretrained for 24 hours forecast\n",
    "      output, output_surface = PanguModel24(input, input_surface)\n",
    "\n",
    "      # Restore from uniformed output\n",
    "      output = output * weather_std + weather_mean\n",
    "      output_surface = output_surface * weather_surface_std + weather_surface_mean\n",
    "\n",
    "      # Stored the output for next round forecast\n",
    "      input_24, input_surface_24 = output, output_surface\n",
    "      input_6, input_surface_6 = output, output_surface\n",
    "      input_3, input_surface_3 = output, output_surface\n",
    "\n",
    "    # switch to the 6-hour model if the forecast time is 30 hours, 36 hours, ..., 24*N + 6/12/18 hours\n",
    "    elif (i+1) % 6 == 0:\n",
    "      # Switch the input back to the stored input\n",
    "      input, input_surface = input_6, input_surface_6\n",
    "\n",
    "      # Call the model pretrained for 6 hours forecast\n",
    "      output, output_surface = PanguModel6(input, input_surface)\n",
    "\n",
    "      # Restore from uniformed output\n",
    "      output = output * weather_std + weather_mean\n",
    "      output_surface = output_surface * weather_surface_std + weather_surface_mean\n",
    "      \n",
    "      # Stored the output for next round forecast\n",
    "      input_6, input_surface_6 = output, output_surface\n",
    "      input_3, input_surface_3 = output, output_surface\n",
    "\n",
    "    # switch to the 3-hour model if the forecast time is 3 hours, 9 hours, ..., 6*N + 3 hours\n",
    "    elif (i+1) % 3 ==0:\n",
    "      # Switch the input back to the stored input\n",
    "      input, input_surface = input_3, input_surface_3\n",
    "\n",
    "      # Call the model pretrained for 3 hours forecast\n",
    "      output, output_surface = PanguModel3(input, input_surface)\n",
    "\n",
    "      # Restore from uniformed output\n",
    "      output = output * weather_std + weather_mean\n",
    "      output_surface = output_surface * weather_surface_std + weather_surface_mean\n",
    "      \n",
    "      # Stored the output for next round forecast\n",
    "      input_3, input_surface_3 = output, output_surface\n",
    "\n",
    "    # switch to the 1-hour model\n",
    "    else:\n",
    "      # Call the model pretrained for 1 hours forecast\n",
    "      output, output_surface = PanguModel1(input, input_surface)\n",
    "\n",
    "      # Restore from uniformed output\n",
    "      output = output * weather_std + weather_mean\n",
    "      output_surface = output_surface * weather_surface_std + weather_surface_mean\n",
    "\n",
    "    # Stored the output for next round forecast\n",
    "    input, input_surface = output, output_surface\n",
    "\n",
    "    # Save the output\n",
    "    output_list.append((output, output_surface))\n",
    "  return output_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def Train(path='PanguModel.pth'):\n",
    "  '''Training code'''\n",
    "  # Initialize the model, for some APIs some adaptation is needed to fit hardwares\n",
    "  model = PanguModel()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=3e-6)\n",
    "  # Train single Pangu-Weather model\n",
    "  epochs = 100\n",
    "  for i in range(epochs):\n",
    "\n",
    "    # dataset_length is the length of your training data, e.g., the sample between 1979 and 2017\n",
    "    for step in range(upper.shape[0]):\n",
    "      # Load weather data at time t as the input; load weather data at time t+1/3/6/24 as the output\n",
    "      # Note the data need to be randomly shuffled\n",
    "      # Note the input and target need to be normalized, see Inference() for details\n",
    "\n",
    "      input, input_surface, target, target_surface = LoadData(step)\n",
    "\n",
    "      # Call the model and get the output\n",
    "      output, output_surface = model(input, input_surface)\n",
    "\n",
    "      # We use the MAE loss to train the model\n",
    "      # The weight of surface loss is 0.25\n",
    "      # Different weight can be applied for differen fields if needed\n",
    "      loss = torch.abs(output - target) + torch.abs(output_surface - target_surface) * 0.25\n",
    "      optimizer.zero_grad()\n",
    "      # loss = TensorAbs(output-target) + TensorAbs(output_surface-target_surface) * 0.25\n",
    "\n",
    "      # Call the backward algorithm and calculate the gratitude of parameters\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "  # Save the model at the end of the training stage\n",
    "  # SaveModel()\n",
    "  ModelPath = path\n",
    "  torch.save(model.state_dict(), ModelPath)\n",
    "  \n",
    "class PanguModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(PanguModel, self).__init__()\n",
    "    # Drop path rate is linearly increased as the depth increases\n",
    "    # drop_path_list = LinearSpace(0, 0.2, 8)\n",
    "    drop_path_list = torch.linspace(0, 0.2, 8)\n",
    "    # Patch embedding\n",
    "    self._input_layer = PatchEmbedding((2, 4, 4), 192)\n",
    "\n",
    "    # Four basic layers\n",
    "    self.layer1 = EarthSpecificLayer(2, 192, drop_path_list[:2], 6,down=False)\n",
    "    self.layer2 = EarthSpecificLayer(6, 384, drop_path_list[:6], 12, down=True)\n",
    "    self.layer3 = EarthSpecificLayer(6, 384, drop_path_list[:6], 12,down=True)\n",
    "    self.layer4 = EarthSpecificLayer(2, 192, drop_path_list[:2], 6, down=False)\n",
    "\n",
    "    # Upsample and downsample\n",
    "    self.upsample = UpSample(384, 192)\n",
    "    self.downsample = DownSample(192)\n",
    "\n",
    "    # Patch Recovery\n",
    "    self._output_layer = PatchRecovery(384,patch_size=(2, 4, 4))\n",
    "    \n",
    "  def forward(self, input, input_surface):\n",
    "    '''Backbone architecture'''\n",
    "    # Embed the input fields into patches\n",
    "    x = self._input_layer(input, input_surface)\n",
    "\n",
    "    # Encoder, composed of two layers\n",
    "    # Layer 1, shape (8, 360, 181, C), C = 192 as in the original paper\n",
    "    x = self.layer1(x, 7, 32, 32) \n",
    "    print('layer1start',x.shape)\n",
    "    # Store the tensor for skip-connection\n",
    "    skip = x\n",
    "\n",
    "    # Downsample from (8, 360, 181) to (8, 180, 91)\n",
    "    x = self.downsample(x, 7, 32, 32)\n",
    "    print('layer2start',x.shape)\n",
    "    # Layer 2, shape (8, 180, 91, 2C), C = 192 as in the original paper\n",
    "    x = self.layer2(x, 7, 16, 16) \n",
    "\n",
    "    # Decoder, composed of two layers\n",
    "    # Layer 3, shape (8, 180, 91, 2C), C = 192 as in the original paper\n",
    "    x = self.layer3(x, 7, 16, 16) \n",
    "\n",
    "    # Upsample from (8, 180, 91) to (8, 360, 181)\n",
    "    x = self.upsample(x)\n",
    "\n",
    "    # Layer 4, shape (8, 360, 181, 2C), C = 192 as in the original paper\n",
    "    x = self.layer4(x, 7, 32, 32) \n",
    "\n",
    "    # Skip connect, in last dimension(C from 192 to 384)\n",
    "    # x = Concatenate(skip, x)\n",
    "    x = torch.cat((x, skip), dim=-1)\n",
    "\n",
    "    # Recover the output fields from patches\n",
    "    output, output_surface = self._output_layer(x,7,32,32)\n",
    "    return output, output_surface\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "  def __init__(self, patch_size, dim):\n",
    "    '''Patch embedding operation'''\n",
    "    super(PatchEmbedding, self).__init__()\n",
    "    # Here we use convolution to partition data into cubes\n",
    "    self.conv = nn.Conv3d(5, dim, kernel_size=patch_size, stride=patch_size)\n",
    "    self.conv_surface = nn.Conv2d(4, dim, kernel_size=patch_size[1:], stride=patch_size[1:])\n",
    "\n",
    "    # Load constant masks from the disc\n",
    "    # self.land_mask, self.soil_type, self.topography = LoadConstantMask()\n",
    "    self.land_mask, self.soil_type, self.topography = torch.tensor(land_masks), torch.tensor(soil_types), torch.tensor(topography)\n",
    "    \n",
    "  def forward(self, input, input_surface):\n",
    "    # Zero-pad the input\n",
    "    # input = Pad3D(input)\n",
    "    input = nn.ZeroPad3d(padding=(0, 0, 0, 0, 0, 0))(input)\n",
    "    # print(input.shape)\n",
    "    # input_surface = Pad2D(input_surface)\n",
    "    input_surface = nn.ZeroPad2d(padding=(0, 0))(input_surface)\n",
    "    # print(input_surface.shape)\n",
    "    # Apply a linear projection for patch_size[0]*patch_size[1]*patch_size[2] patches, patch_size = (2, 4, 4) as in the original paper\n",
    "    input = self.conv(input)\n",
    "    # print('input:',input.shape)\n",
    "    # Add three constant fields to the surface fields\n",
    "    # input_surface =  Concatenate(input_surface, self.land_mask, self.soil_type, self.topography)\n",
    "    land_mask_expanded = self.land_mask.unsqueeze(0).unsqueeze(0)  # (1, 1, 721, 1440)\n",
    "    soil_type_expanded = self.soil_type.unsqueeze(0).unsqueeze(0)  \n",
    "    topography_expanded = self.topography.unsqueeze(0).unsqueeze(0)  \n",
    "    # Add the expanded fields to input_surface\n",
    "    input_surface = input_surface + land_mask_expanded + soil_type_expanded + topography_expanded\n",
    "    # Apply a linear projection for patch_size[1]*patch_size[2] patches\n",
    "    # print(input_surface.shape)\n",
    "    input_surface = self.conv_surface(input_surface)\n",
    "    # print('surface:',input_surface.shape)\n",
    "    # Concatenate the input in the pressure level, i.e., in Z dimension\n",
    "    # x = Concatenate(input, input_surface)\n",
    "    \n",
    "    x = torch.cat((input, input_surface.unsqueeze(2)), dim=2)\n",
    "    # print(x.shape)\n",
    "    #x (B, C, Z, H, W)\n",
    "    # Reshape x for calculation of linear projections\n",
    "    # x = TransposeDimensions(x, (0, 2, 3, 4, 1))\n",
    "    x = x.permute(0, 2, 3, 4, 1) #channel first to channel last\n",
    "    # print(x.shape)\n",
    "    x = x.reshape(x.shape[0], 7*32*32, x.shape[-1])\n",
    "    return x\n",
    "    \n",
    " \n",
    "class PatchRecovery(nn.Module):\n",
    "  def __init__(self, dim,patch_size):\n",
    "    '''Patch recovery operation'''\n",
    "    super(PatchRecovery, self).__init__()\n",
    "    # Hear we use two transposed convolutions to recover data\n",
    "    self.conv = nn.ConvTranspose3d(dim, 5, kernel_size=patch_size, stride=patch_size)\n",
    "    self.conv_surface = nn.ConvTranspose2d(dim, 4, kernel_size=patch_size[1:], stride=patch_size[1:])\n",
    "    \n",
    "  def forward(self, x, Z, H, W):\n",
    "    # The inverse operation of the patch embedding operation, patch_size = (2, 4, 4) as in the original paper\n",
    "    # Reshape x back to three dimensions\n",
    "    x = x.permute(0, 2, 1)\n",
    "    # x = reshape(x, target_shape=(x.shape[0], x.shape[1], Z, H, W))\n",
    "    x = x.reshape(x.shape[0], x.shape[1], Z, H, W)\n",
    "\n",
    "    # Call the transposed convolution\n",
    "    output = self.conv(x[:, :, 1:, :, :])\n",
    "    output_surface = self.conv_surface(x[:, :, 0, :, :])\n",
    "\n",
    "    # Crop the output to remove zero-paddings\n",
    "    # output = Crop3D(output)\n",
    "    # output_surface = Crop2D(output_surface)\n",
    "    return output, output_surface\n",
    "\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "  def __init__(self, dim):\n",
    "    '''Down-sampling operation'''\n",
    "    super(DownSample, self).__init__()\n",
    "    # A linear function and a layer normalization\n",
    "    self.linear = nn.Linear(4*dim, 2*dim, bias=False)\n",
    "    self.norm = nn.LayerNorm(4*dim)\n",
    "  \n",
    "  def forward(self, x, Z, H, W):\n",
    "    # Reshape x to three dimensions for downsampling\n",
    "    # x = reshape(x, target_shape=(x.shape[0], Z, H, W, x.shape[-1]))\n",
    "\n",
    "    x = x.reshape(x.shape[0], Z, H, W, x.shape[-1])\n",
    "    # Padding the input to facilitate downsampling\n",
    "    # x = Pad3D(x)\n",
    "    print(\"down x\", x.shape)\n",
    "    # Reorganize x to reduce the resolution: simply change the order and downsample from (8, 360, 182) to (8, 180, 91)\n",
    "    # Z, H, W = x.shape[-3,-2,-1]\n",
    "    # Reshape x to facilitate downsampling\n",
    "    # x = reshape(x, target_shape=(x.shape[0], Z, H//2, 2, W//2, 2, x.shape[-1]))\n",
    "    x = x.reshape(x.shape[0], Z, H//2, 2, W//2, 2, x.shape[-1])\n",
    "    # Change the order of x\n",
    "    # x = TransposeDimensions(x, (0,1,2,4,3,5,6))\n",
    "    x = x.permute(0, 1, 2, 4, 3, 5, 6)\n",
    "    # Reshape to get a tensor of resolution (8, 180, 91)\n",
    "    # x = reshape(x, target_shape=(x.shape[0], Z*(H//2)*(W//2), 4 * x.shape[-1]))\n",
    "    x = x.reshape(x.shape[0], Z*(H//2)*(W//2), 4 * x.shape[-1])\n",
    "    # Call the layer normalization\n",
    "    x = self.norm(x)\n",
    "\n",
    "    # Decrease the channels of the data to reduce computation cost\n",
    "    x = self.linear(x)\n",
    "    return x\n",
    "class UpSample(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "    '''Up-sampling operation'''\n",
    "    super(UpSample, self).__init__()\n",
    "    # Linear layers without bias to increase channels of the data\n",
    "    self.linear1 = nn.Linear(input_dim, output_dim*4, bias=False)\n",
    "\n",
    "    # Linear layers without bias to mix the data up\n",
    "    self.linear2 = nn.Linear(output_dim, output_dim, bias=False)\n",
    "\n",
    "    # Normalization\n",
    "    self.norm = nn.LayerNorm(output_dim)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    # Call the linear functions to increase channels of the data\n",
    "    x = self.linear1(x)\n",
    "\n",
    "    # Reorganize x to increase the resolution: simply change the order and upsample from (8, 180, 91) to (8, 360, 182)\n",
    "    # Reshape x to facilitate upsampling.\n",
    "    # x = reshape(x, target_shape=(x.shape[0], 8, 180, 91, 2, 2, x.shape[-1]//4))\n",
    "    x = x.reshape(x.shape[0], 7, 16,16, 2, 2, x.shape[-1]//4)\n",
    "    # Change the order of x\n",
    "    # x = TransposeDimensions(x, (0,1,2,4,3,5,6))\n",
    "    x = x.permute(0, 1, 2, 4, 3, 5, 6)\n",
    "    # Reshape to get Tensor with a resolution of (8, 360, 182)\n",
    "    # x = reshape(x, target_shape=(x.shape[0], 8, 360, 182, x.shape[-1]))\n",
    "    x = x.reshape(x.shape[0], 7, 32, 32, x.shape[-1])\n",
    "    # Crop the output to the input shape of the network\n",
    "    # x = Crop3D(x)\n",
    "\n",
    "    # Reshape x back\n",
    "    # x = reshape(x, target_shape=(x.shape[0], x.shape[1]*x.shape[2]*x.shape[3], x.shape[-1]))\n",
    "    x = torch.reshape(x, (x.shape[0], x.shape[1]*x.shape[2]*x.shape[3], x.shape[-1]))\n",
    "    # Call the layer normalization\n",
    "    x = self.norm(x)\n",
    "\n",
    "    # Mixup normalized tensors\n",
    "    x = self.linear2(x)\n",
    "    return x\n",
    "  \n",
    "class EarthSpecificLayer(nn.Module):\n",
    "  def __init__(self, depth, dim, drop_path_ratio_list, heads,down):\n",
    "    '''Basic layer of our network, contains 2 or 6 blocks'''\n",
    "    super(EarthSpecificLayer, self).__init__()\n",
    "    self.depth = depth\n",
    "    self.blocks = []\n",
    "\n",
    "    # Construct basic blocks\n",
    "    for i in range(depth):\n",
    "      self.blocks.append(EarthSpecificBlock(dim, drop_path_ratio_list[i], heads,down))\n",
    "      \n",
    "  def forward(self, x, Z, H, W):\n",
    "    for i in range(self.depth):\n",
    "      # Roll the input every two blocks\n",
    "      if i % 2 == 0:\n",
    "        self.blocks[i](x, Z, H, W, roll=False)\n",
    "      else:\n",
    "        self.blocks[i](x, Z, H, W, roll=True)\n",
    "    return x\n",
    "\n",
    "class EarthSpecificBlock(nn.Module):\n",
    "  def __init__(self, dim, drop_path_ratio, heads,down):\n",
    "    '''\n",
    "    3D transformer block with Earth-Specific bias and window attention, \n",
    "    see https://github.com/microsoft/Swin-Transformer for the official implementation of 2D window attention.\n",
    "    The major difference is that we expand the dimensions to 3 and replace the relative position bias with Earth-Specific bias.\n",
    "    '''\n",
    "    super(EarthSpecificBlock, self).__init__()\n",
    "    # Define the window size of the neural network \n",
    "    self.window_size = (1, 4, 8)\n",
    "\n",
    "    # Initialize serveral operations\n",
    "    self.drop_path = DropPath(drop_path_ratio)\n",
    "    # self.norm1 = LayerNorm(dim)\n",
    "    self.norm1 = nn.LayerNorm(dim)\n",
    "    self.norm2 = nn.LayerNorm(dim)\n",
    "    # self.norm2 = LayerNorm(dim)\n",
    "    self.linear = Mlp(dim, 0)\n",
    "    self.attention = EarthAttention3D(dim, heads, 0, self.window_size,down)\n",
    "\n",
    "  def roll3D(self,x, shift):\n",
    "    assert len(shift) == 3, \"Shift must specify three dimensions: (Z, H, W)\"\n",
    "    Z_shift, H_shift, W_shift = shift\n",
    "\n",
    "    # Roll along each dimension\n",
    "    if Z_shift != 0:\n",
    "        x = torch.roll(x, shifts=Z_shift, dims=1)  \n",
    "    if H_shift != 0:\n",
    "        x = torch.roll(x, shifts=H_shift, dims=2)  \n",
    "    if W_shift != 0:\n",
    "        x = torch.roll(x, shifts=W_shift, dims=3)  \n",
    "    return x\n",
    "\n",
    "  def gen_mask(self, x):\n",
    "\n",
    "    B, Z, H, W, C = x.shape\n",
    "    Z_win, H_win, W_win = self.window_size\n",
    "\n",
    "    # Calculate the number of windows along each dimension\n",
    "    num_Z = Z // Z_win\n",
    "    num_H = H // H_win\n",
    "    num_W = W // W_win\n",
    "    num_windows = num_Z * num_H * num_W\n",
    "    num_tokens = Z_win * H_win * W_win\n",
    "\n",
    "    # Initialize \n",
    "    mask = torch.zeros((B, num_windows, num_tokens, num_tokens), device=x.device)\n",
    "\n",
    "    # block non-adjacent positions\n",
    "    for z in range(Z_win):\n",
    "        for h in range(H_win):\n",
    "            for w in range(W_win):\n",
    "                token_id_1 = z * H_win * W_win + h * W_win + w\n",
    "                for z2 in range(Z_win):\n",
    "                    for h2 in range(H_win):\n",
    "                        for w2 in range(W_win):\n",
    "                            token_id_2 = z2 * H_win * W_win + h2 * W_win + w2\n",
    "                            if abs(z - z2) > 1 or abs(h - h2) > 1 or abs(w - w2) > 1:\n",
    "                                mask[:, :, token_id_1, token_id_2] = -1000  # Block non-adjacent tokens\n",
    "    return mask\n",
    "\n",
    "\n",
    "  def forward(self, x, Z, H, W, roll):\n",
    "    # Save the shortcut for skip-connection\n",
    "    shortcut = x\n",
    "\n",
    "    # Reshape input to three dimensions to calculate window attention\n",
    "    # reshape(x, target_shape=(x.shape[0], Z, H, W, x.shape[2]))\n",
    "    x = x.reshape(x.shape[0], Z, H, W, x.shape[2])\n",
    "    # x = pad3D(x)\n",
    "\n",
    "    # Store the shape of the input for restoration\n",
    "    ori_shape = x.shape\n",
    "\n",
    "    if roll:\n",
    "      # Roll x for half of the window for 3 dimensions\n",
    "      x = self.roll3D(x, shift=[self.window_size[0]//2, self.window_size[1]//2, self.window_size[2]//2])\n",
    "      # Generate mask of attention masks\n",
    "      # If two pixels are not adjacent, then mask the attention between them\n",
    "      # Your can set the matrix element to -1000 when it is not adjacent, then add it to the attention\n",
    "      mask = self.gen_mask(x)\n",
    "    else:\n",
    "      # e.g., zero matrix when you add mask to attention\n",
    "      mask = torch.zeros_like(x)\n",
    "      \n",
    "    print('x_layer',x.shape)\n",
    "    # Reorganize data to calculate window attention\n",
    "    # x_window = reshape(x, target_shape=(x.shape[0], Z//window_size[0], window_size[0], H // window_size[1], window_size[1], W // window_size[2], window_size[2], x.shape[-1]))\n",
    "    x_window = x.reshape(x.shape[0], Z//self.window_size[0], self.window_size[0], H // self.window_size[1], self.window_size[1], W // self.window_size[2], self.window_size[2], x.shape[-1])\n",
    "\n",
    "    # x_window = TransposeDimensions(x_window, (0, 1, 3, 5, 2, 4, 6, 7))\n",
    "    x_window = x_window.permute(0, 1, 3, 5, 2, 4, 6, 7)\n",
    "    # Get data stacked in 3D cubes, which will further be used to calculated attention among each cube\n",
    "    # x_window = reshape(x_window, target_shape=(-1, window_size[0]* window_size[1]*window_size[2], x.shape[-1]))\n",
    "    x_window = x_window.reshape(-1,self.window_size[0]*self.window_size[1]*self.window_size[2], x.shape[-1])\n",
    "    # Apply 3D window attention with Earth-Specific bias\n",
    "    print('window',x_window.shape)\n",
    "    x_window = self.attention(x_window, mask)\n",
    "\n",
    "    # Reorganize data to original shapes\n",
    "    # x = reshape(x_window, target_shape=((-1, Z // window_size[0], H // window_size[1], W // window_size[2], window_size[0], window_size[1], window_size[2], x_window.shape[-1])))\n",
    "    x = x_window.reshape(-1,Z//self.window_size[0],H//self.window_size[1],W//self.window_size[2],self.window_size[0],self.window_size[1],self.window_size[2],x_window.shape[-1])\n",
    "    # x = TransposeDimensions(x, (0, 1, 4, 2, 5, 3, 6, 7))\n",
    "    x = x.permute(0, 1, 4, 2, 5, 3, 6, 7)\n",
    "    # Reshape the tensor back to its original shape\n",
    "    # x = reshape(x_window, target_shape=ori_shape)\n",
    "    x = x.reshape(ori_shape)\n",
    "    if roll:\n",
    "      # Roll x back for half of the window\n",
    "      x = self.roll3D(x, shift=[-self.window_size[0]//2, -self.window_size[1]//2, -self.window_size[2]//2])\n",
    "\n",
    "    # # Crop the zero-padding\n",
    "    # x = Crop3D(x)\n",
    "\n",
    "    # Reshape the tensor back to the input shape\n",
    "    # x = reshape(x, target_shape=(x.shape[0], x.shape[1]*x.shape[2]*x.shape[3], x.shape[4]))\n",
    "    x = x.reshape(x.shape[0], x.shape[1]* x.shape[2]* x.shape[3], x.shape[4])\n",
    "    # Main calculation stages\n",
    "    x = shortcut + self.drop_path(self.norm1(x))\n",
    "    x = x + self.drop_path(self.norm2(self.linear(x)))\n",
    "    return x\n",
    "    \n",
    "class EarthAttention3D(nn.Module):\n",
    "  def __init__(self, dim, heads, dropout_rate, window_size, down):\n",
    "    super(EarthAttention3D, self).__init__()\n",
    "    '''\n",
    "    3D window attention with the Earth-Specific bias, \n",
    "    see https://github.com/microsoft/Swin-Transformer for the official implementation of 2D window attention.\n",
    "    '''\n",
    "    # Initialize several operations\n",
    "    self.linear1 = nn.Linear(dim,3*dim, bias=True) #creat qkv\n",
    "    self.linear2 = nn.Linear(dim, dim)\n",
    "    self.softmax = nn.Softmax(dim=-1)\n",
    "    self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    # Store several attributes\n",
    "    self.head_number = heads\n",
    "    self.dim = dim\n",
    "    self.scale = (dim//heads)**-0.5\n",
    "    self.window_size = window_size\n",
    "\n",
    "    # input_shape is current shape of the self.forward function\n",
    "    # You can run your code to record it, modify the code and rerun it\n",
    "    # Record the number of different window types\n",
    "\n",
    "    # self.type_of_windows = (input_shape[0]//window_size[0])*(input_shape[1]//window_size[1]), (126//window_size[2])*(50//window_size[1])\n",
    "    self.type_of_windows = 56 if down else 224\n",
    "\n",
    "    # For each type of window, we will construct a set of parameters according to the paper\n",
    "    # self.earth_specific_bias = ConstructTensor(shape=((2 * window_size[2] - 1) * window_size[1] * window_size[1] * window_size[0] * window_size[0], self.type_of_windows, heads))\n",
    "    # self.earth_specific_bias = Parameters(self.earth_specific_bias)\n",
    "    self.earth_specific_bias = nn.Parameter(\n",
    "            torch.randn(\n",
    "                (2 * window_size[2] - 1) * window_size[1] * window_size[1] * window_size[0] * window_size[0],\n",
    "                self.type_of_windows,\n",
    "                heads\n",
    "            )\n",
    "        )\n",
    "    # Initialize the tensors using Truncated normal distribution\n",
    "    # TruncatedNormalInit(self.earth_specific_bias, std=0.02) \n",
    "    nn.init.trunc_normal_(self.earth_specific_bias, std=0.02)\n",
    "    # Construct position index to reuse self.earth_specific_bias\n",
    "    self.position_index = self._construct_index()\n",
    "    \n",
    "  def _construct_index(self):\n",
    "    ''' This function construct the position index to reuse symmetrical parameters of the position bias'''\n",
    "    coords_zi = torch.arange(self.window_size[0])\n",
    "    coords_zj = -coords_zi * self.window_size[0]\n",
    "\n",
    "    coords_hi = torch.arange(self.window_size[1])\n",
    "    coords_hj = -coords_hi * self.window_size[1]\n",
    "\n",
    "    coords_w = torch.arange(self.window_size[2])\n",
    "\n",
    "    coords_1 = torch.stack(torch.meshgrid(coords_zi, coords_hi, coords_w, indexing='ij'), dim=-1).view(-1, 3)\n",
    "    coords_2 = torch.stack(torch.meshgrid(coords_zj, coords_hj, coords_w, indexing='ij'), dim=-1).view(-1, 3)\n",
    "\n",
    "    coords = coords_1[:, None, :] - coords_2[None, :, :]\n",
    "    coords = coords.transpose(0, 1)\n",
    "\n",
    "    coords[:, :, 2] += self.window_size[2] - 1\n",
    "    coords[:, :, 1] *= 2 * self.window_size[2] - 1\n",
    "    coords[:, :, 0] *= (2 * self.window_size[2] - 1) * self.window_size[1] * self.window_size[1]\n",
    "\n",
    "    position_index = coords.sum(dim=-1).view(-1)\n",
    "    return position_index\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    # Linear layer to create query, key and value\n",
    "    original_shape = x.shape\n",
    "    x = self.linear1(x)\n",
    "\n",
    "    # Record the original shape of the input\n",
    "    \n",
    "\n",
    "    # reshape the data to calculate multi-head attention\n",
    "    # qkv = reshape(x, target_shape=(x.shape[0], x.shape[1], 3, self.head_number, self.dim // self.head_number)) \n",
    "    qkv = x.reshape(x.shape[0], x.shape[1], 3, self.head_number, self.dim // self.head_number)\n",
    "    # query, key, value = TransposeDimensions(qkv, (2, 0, 3, 1, 4))\n",
    "    query, key, value = qkv.permute(2, 0, 3, 1, 4)\n",
    "    # Scale the attention\n",
    "    query = query * self.scale\n",
    "    print('query',query.shape)\n",
    "    # Calculated the attention, a learnable bias is added to fix the nonuniformity of the grid.\n",
    "    # attention = query @ key.T # @ denotes matrix multiplication\n",
    "    attention = torch.matmul(query, key.transpose(-2, -1))\n",
    "\n",
    "    # self.earth_specific_bias is a set of neural network parameters to optimize. \n",
    "    EarthSpecificBias = self.earth_specific_bias[self.position_index]\n",
    "\n",
    "    # Reshape the learnable bias to the same shape as the attention matrix\n",
    "    # EarthSpecificBias = reshape(EarthSpecificBias, target_shape=(self.window_size[0]*self.window_size[1]*self.window_size[2], self.window_size[0]*self.window_size[1]*self.window_size[2], self.type_of_windows, self.head_number))\n",
    "    EarthSpecificBias = EarthSpecificBias.reshape(self.window_size[0]*self.window_size[1]*self.window_size[2], self.window_size[0]*self.window_size[1]*self.window_size[2], self.type_of_windows, self.head_number)\n",
    "    EarthSpecificBias = EarthSpecificBias.permute(2,3,0,1)\n",
    "    # EarthSpecificBias = TransposeDimensions(EarthSpecificBias, (2, 3, 0, 1))\n",
    "\n",
    "    # EarthSpecificBias = reshape(EarthSpecificBias, target_shape = [1]+EarthSpecificBias.shape)\n",
    "    EarthSpecificBias = EarthSpecificBias.unsqueeze(0)\n",
    "    # Add the Earth-Specific bias to the attention matrix\n",
    "    print('attention',attention.shape,'EarthSpecificBias',EarthSpecificBias.shape)\n",
    "    attention = attention + EarthSpecificBias\n",
    "    print('attention',attention.shape)\n",
    "    print(mask.shape)\n",
    "\n",
    "    # Mask the attention between non-adjacent pixels, e.g., simply add -100 to the masked element.\n",
    "    # attention = self.mask_attention(attention, mask)\n",
    "    # attention = self.softmax(attention)\n",
    "    # attention = self.dropout(attention)\n",
    "\n",
    "    print('attention',attention.shape)\n",
    "    # Calculated the tensor after spatial mixing.\n",
    "    # x = attention @ value.T # @ denote matrix multiplication\n",
    "    x = torch.matmul(attention, value.unsqueeze(0))\n",
    "    print('x',x.shape)\n",
    "    # Reshape tensor to the original shape\n",
    "    # x = TransposeDimensions(x, (0, 2, 1))\n",
    "    # [1, 126, 6, 50, 32]\n",
    "    x = x.reshape(x.shape[1],x.shape[-2],-1)\n",
    "    # x = x.permute(0,2,1)\n",
    "    # x = reshape(x, target_shape = original_shape)\n",
    "    print(original_shape)\n",
    "    x = x.reshape(original_shape)\n",
    "    # Linear layer to post-process operated tensor\n",
    "    x = self.linear2(x)\n",
    "    x = self.dropout(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "  def mask_attention(self, attention, mask):\n",
    "        if mask is not None:\n",
    "            attention += mask.unsqueeze(1).unsqueeze(2) * -100\n",
    "        return attention\n",
    "  \n",
    "class Mlp(nn.Module):\n",
    "  def __init__(self, dim, dropout_rate):\n",
    "    super(Mlp, self).__init__()\n",
    "    self.linear1 = nn.Linear(dim, dim * 4)\n",
    "    self.linear2 = nn.Linear(dim * 4, dim)\n",
    "    self.activation = nn.GELU()\n",
    "    self.drop = nn.Dropout(dropout_rate)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = self.linear1(x)\n",
    "    x = self.activation(x)\n",
    "    x = self.drop(x)\n",
    "    x = self.linear2(x)\n",
    "    x = self.drop(x)\n",
    "    return x\n",
    "  \n",
    "from perlin_numpy import generate_fractal_noise_2d\n",
    "def PerlinNoise():\n",
    "  '''Generate random Perlin noise: we follow https://github.com/pvigier/perlin-numpy/ to calculate the perlin noise.'''\n",
    "  # Define number of noise\n",
    "  octaves = 3\n",
    "  # Define the scaling factor of noise\n",
    "  noise_scale = 0.2\n",
    "  # Define the number of periods of noise along the axis\n",
    "  period_number = 12\n",
    "  # The size of an input slice\n",
    "  H, W = 128, 128\n",
    "  # Scaling factor between two octaves\n",
    "  persistence = 0.5\n",
    "  # perlin_noise = noise_scale*GenerateFractalNoise((H, W), (period_number, period_number), octaves, persistence)\n",
    "  perlin_noise = noise_scale*generate_fractal_noise_2d((H,W), (period_number, period_number), octaves, persistence)\n",
    "  return perlin_noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class PatchEmbedding(nn.Module):\n",
    "#   def __init__(self, patch_size, dim):\n",
    "#     '''Patch embedding operation'''\n",
    "#     super(PatchEmbedding, self).__init__()\n",
    "#     # Here we use convolution to partition data into cubes\n",
    "#     self.conv = nn.Conv3d(5, dim, kernel_size=patch_size, stride=patch_size)\n",
    "#     self.conv_surface = nn.Conv2d(4, dim, kernel_size=patch_size[1:], stride=patch_size[1:])\n",
    "\n",
    "#     # Load constant masks from the disc\n",
    "#     # self.land_mask, self.soil_type, self.topography = LoadConstantMask()\n",
    "#     self.land_mask, self.soil_type, self.topography = torch.tensor(land_masks), torch.tensor(soil_types), torch.tensor(topography)\n",
    "    \n",
    "#   def forward(self, input, input_surface):\n",
    "#     # Zero-pad the input\n",
    "#     # input = Pad3D(input)\n",
    "#     input = nn.ZeroPad3d(padding=(0, 0, 0, 0, 0, 0))(input)\n",
    "#     print(input.shape)\n",
    "#     # input_surface = Pad2D(input_surface)\n",
    "#     input_surface = nn.ZeroPad2d(padding=(0, 0))(input_surface)\n",
    "#     # print(input_surface.shape)\n",
    "#     # Apply a linear projection for patch_size[0]*patch_size[1]*patch_size[2] patches, patch_size = (2, 4, 4) as in the original paper\n",
    "#     input = self.conv(input)\n",
    "#     print('input:',input.shape)\n",
    "#     # Add three constant fields to the surface fields\n",
    "#     # input_surface =  Concatenate(input_surface, self.land_mask, self.soil_type, self.topography)\n",
    "#     land_mask_expanded = self.land_mask.unsqueeze(0).unsqueeze(0)  # (1, 1, 721, 1440)\n",
    "#     soil_type_expanded = self.soil_type.unsqueeze(0).unsqueeze(0)  \n",
    "#     topography_expanded = self.topography.unsqueeze(0).unsqueeze(0)  \n",
    "#     # Add the expanded fields to input_surface\n",
    "#     input_surface = input_surface + land_mask_expanded + soil_type_expanded + topography_expanded\n",
    "#     # Apply a linear projection for patch_size[1]*patch_size[2] patches\n",
    "#     print(input_surface.shape)\n",
    "#     input_surface = self.conv_surface(input_surface)\n",
    "#     print('surface:',input_surface.shape)\n",
    "#     # Concatenate the input in the pressure level, i.e., in Z dimension\n",
    "#     # x = Concatenate(input, input_surface)\n",
    "    \n",
    "#     x = torch.cat((input_surface.unsqueeze(2),input), dim=2)\n",
    "#     print(x.shape)\n",
    "#     #x (B, C, Z, H, W)\n",
    "#     # Reshape x for calculation of linear projections\n",
    "#     # x = TransposeDimensions(x, (0, 2, 3, 4, 1))\n",
    "#     x = x.permute(0, 2, 3, 4, 1) #channel first to channel last\n",
    "#     print(x.shape)\n",
    "#     x = x.reshape(x.shape[0], 7*32*32, x.shape[-1])\n",
    "#     return x\n",
    "    \n",
    "  \n",
    "# class PatchRecovery(nn.Module):\n",
    "#   def __init__(self, dim,patch_size):\n",
    "#     '''Patch recovery operation'''\n",
    "#     super(PatchRecovery, self).__init__()\n",
    "#     # Hear we use two transposed convolutions to recover data\n",
    "#     self.conv = nn.ConvTranspose3d(dim, 5, kernel_size=patch_size, stride=patch_size)\n",
    "#     self.conv_surface = nn.ConvTranspose2d(dim, 4, kernel_size=patch_size[1:], stride=patch_size[1:])\n",
    "    \n",
    "#   def forward(self, x, Z, H, W):\n",
    "#     # The inverse operation of the patch embedding operation, patch_size = (2, 4, 4) as in the original paper\n",
    "#     # Reshape x back to three dimensions\n",
    "#     x = x.permute(0, 2, 1)\n",
    "#     # x = reshape(x, target_shape=(x.shape[0], x.shape[1], Z, H, W))\n",
    "#     x = x.reshape(x.shape[0], x.shape[1], Z, H, W)\n",
    "\n",
    "#     # Call the transposed convolution\n",
    "#     output = self.conv(x[:, :, 1:, :, :])\n",
    "#     output_surface = self.conv_surface(x[:, :, 0, :, :])\n",
    "\n",
    "#     # Crop the output to remove zero-paddings\n",
    "#     # output = Crop3D(output)\n",
    "#     # output_surface = Crop2D(output_surface)\n",
    "#     return output, output_surface\n",
    "\n",
    "\n",
    "\n",
    "# class DownSample(nn.Module):\n",
    "#   def __init__(self, dim):\n",
    "#     '''Down-sampling operation'''\n",
    "#     super(DownSample, self).__init__()\n",
    "#     # A linear function and a layer normalization\n",
    "#     self.linear = nn.Linear(4*dim, 2*dim, bias=False)\n",
    "#     self.norm = nn.LayerNorm(4*dim)\n",
    "  \n",
    "#   def forward(self, x, Z, H, W):\n",
    "#     # Reshape x to three dimensions for downsampling\n",
    "#     # x = reshape(x, target_shape=(x.shape[0], Z, H, W, x.shape[-1]))\n",
    "\n",
    "#     x = x.reshape(x.shape[0], Z, H, W, x.shape[-1])\n",
    "#     # Padding the input to facilitate downsampling\n",
    "#     # x = Pad3D(x)\n",
    "#     print(\"down x\", x.shape)\n",
    "#     # Reorganize x to reduce the resolution: simply change the order and downsample from (8, 360, 182) to (8, 180, 91)\n",
    "#     # Z, H, W = x.shape[-3,-2,-1]\n",
    "#     # Reshape x to facilitate downsampling\n",
    "#     # x = reshape(x, target_shape=(x.shape[0], Z, H//2, 2, W//2, 2, x.shape[-1]))\n",
    "#     x = x.reshape(x.shape[0], Z, H//2, 2, W//2, 2, x.shape[-1])\n",
    "#     # Change the order of x\n",
    "#     # x = TransposeDimensions(x, (0,1,2,4,3,5,6))\n",
    "#     x = x.permute(0, 1, 2, 4, 3, 5, 6)\n",
    "#     # Reshape to get a tensor of resolution (8, 180, 91)\n",
    "#     # x = reshape(x, target_shape=(x.shape[0], Z*(H//2)*(W//2), 4 * x.shape[-1]))\n",
    "#     x = x.reshape(x.shape[0], Z*(H//2)*(W//2), 4 * x.shape[-1])\n",
    "#     # Call the layer normalization\n",
    "#     x = self.norm(x)\n",
    "\n",
    "#     # Decrease the channels of the data to reduce computation cost\n",
    "#     x = self.linear(x)\n",
    "#     return x\n",
    "\n",
    "# class UpSample(nn.Module):\n",
    "#   def __init__(self, input_dim, output_dim):\n",
    "#     '''Up-sampling operation'''\n",
    "#     super(UpSample, self).__init__()\n",
    "#     # Linear layers without bias to increase channels of the data\n",
    "#     self.linear1 = nn.Linear(input_dim, output_dim*4, bias=False)\n",
    "\n",
    "#     # Linear layers without bias to mix the data up\n",
    "#     self.linear2 = nn.Linear(output_dim, output_dim, bias=False)\n",
    "\n",
    "#     # Normalization\n",
    "#     self.norm = nn.LayerNorm(output_dim)\n",
    "  \n",
    "#   def forward(self, x):\n",
    "#     # Call the linear functions to increase channels of the data\n",
    "#     x = self.linear1(x)\n",
    "\n",
    "#     # Reorganize x to increase the resolution: simply change the order and upsample from (8, 180, 91) to (8, 360, 182)\n",
    "#     # Reshape x to facilitate upsampling.\n",
    "#     # x = reshape(x, target_shape=(x.shape[0], 8, 180, 91, 2, 2, x.shape[-1]//4))\n",
    "#     x = x.reshape(x.shape[0], 7, 15,15, 2, 2, x.shape[-1]//4)\n",
    "#     # Change the order of x\n",
    "#     # x = TransposeDimensions(x, (0,1,2,4,3,5,6))\n",
    "#     x = x.permute(0, 1, 2, 4, 3, 5, 6)\n",
    "#     # Reshape to get Tensor with a resolution of (8, 360, 182)\n",
    "#     # x = reshape(x, target_shape=(x.shape[0], 8, 360, 182, x.shape[-1]))\n",
    "#     x = x.reshape(x.shape[0], 7, 30, 30, x.shape[-1])\n",
    "#     # Crop the output to the input shape of the network\n",
    "#     # x = Crop3D(x)\n",
    "\n",
    "#     # Reshape x back\n",
    "#     # x = reshape(x, target_shape=(x.shape[0], x.shape[1]*x.shape[2]*x.shape[3], x.shape[-1]))\n",
    "#     x = torch.reshape(x, (x.shape[0], x.shape[1]*x.shape[2]*x.shape[3], x.shape[-1]))\n",
    "#     # Call the layer normalization\n",
    "#     x = self.norm(x)\n",
    "\n",
    "#     # Mixup normalized tensors\n",
    "#     x = self.linear2(x)\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "EarthSpecificLayer.__init__() missing 1 required positional argument: 'down'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m up \u001b[38;5;241m=\u001b[39m UpSample(\u001b[38;5;241m384\u001b[39m,\u001b[38;5;241m192\u001b[39m)\n\u001b[1;32m      5\u001b[0m drop_path_list \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m layer1 \u001b[38;5;241m=\u001b[39m \u001b[43mEarthSpecificLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_path_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m upper \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m128\u001b[39m,dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      9\u001b[0m surface \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m128\u001b[39m,dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mTypeError\u001b[0m: EarthSpecificLayer.__init__() missing 1 required positional argument: 'down'"
     ]
    }
   ],
   "source": [
    "# embedding = PatchEmbedding((2, 4, 4), 192)\n",
    "# recovery = PatchRecovery(384,(2, 4, 4))\n",
    "# down = DownSample(192)\n",
    "# up = UpSample(384,192)\n",
    "# drop_path_list = torch.linspace(0, 0.2, 8)\n",
    "\n",
    "# layer1 = EarthSpecificLayer(2, 192, drop_path_list[:2], 6)\n",
    "# upper = torch.rand(1,5,12,128,128,dtype=torch.float32)\n",
    "# surface = torch.rand(1,4,128,128,dtype=torch.float32)\n",
    "\n",
    "# x= embedding(upper,surface)\n",
    "# print(\"x:\",x.shape)\n",
    "# skip = x\n",
    "# x = layer1(x, 7, 32, 32) \n",
    "# x = down(x,7,32,32)\n",
    "# print(\"down x:\",x.shape)\n",
    "# x = up(x)\n",
    "# print(\"up x:\",x.shape)\n",
    "# x = torch.cat([x,skip],dim=-1)\n",
    "# x = recovery(x,7,30,30)\n",
    "# print(x[0].shape,x[1].shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_layer torch.Size([1, 7, 32, 32, 192])\n",
      "window torch.Size([224, 32, 192])\n",
      "query torch.Size([224, 6, 32, 32])\n",
      "attention torch.Size([224, 6, 32, 32]) EarthSpecificBias torch.Size([1, 224, 6, 32, 32])\n",
      "attention torch.Size([1, 224, 6, 32, 32])\n",
      "torch.Size([1, 7, 32, 32, 192])\n",
      "attention torch.Size([1, 224, 6, 32, 32])\n",
      "x torch.Size([1, 224, 6, 32, 32])\n",
      "torch.Size([224, 32, 192])\n",
      "x_layer torch.Size([1, 7, 32, 32, 192])\n",
      "window torch.Size([224, 32, 192])\n",
      "query torch.Size([224, 6, 32, 32])\n",
      "attention torch.Size([224, 6, 32, 32]) EarthSpecificBias torch.Size([1, 224, 6, 32, 32])\n",
      "attention torch.Size([1, 224, 6, 32, 32])\n",
      "torch.Size([1, 224, 32, 32])\n",
      "attention torch.Size([1, 224, 6, 32, 32])\n",
      "x torch.Size([1, 224, 6, 32, 32])\n",
      "torch.Size([224, 32, 192])\n",
      "layer1start torch.Size([1, 7168, 192])\n",
      "down x torch.Size([1, 7, 32, 32, 192])\n",
      "layer2start torch.Size([1, 1792, 384])\n",
      "x_layer torch.Size([1, 7, 16, 16, 384])\n",
      "window torch.Size([56, 32, 384])\n",
      "query torch.Size([56, 12, 32, 32])\n",
      "attention torch.Size([56, 12, 32, 32]) EarthSpecificBias torch.Size([1, 56, 12, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([1, 7, 16, 16, 384])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "x torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([56, 32, 384])\n",
      "x_layer torch.Size([1, 7, 16, 16, 384])\n",
      "window torch.Size([56, 32, 384])\n",
      "query torch.Size([56, 12, 32, 32])\n",
      "attention torch.Size([56, 12, 32, 32]) EarthSpecificBias torch.Size([1, 56, 12, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([1, 56, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "x torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([56, 32, 384])\n",
      "x_layer torch.Size([1, 7, 16, 16, 384])\n",
      "window torch.Size([56, 32, 384])\n",
      "query torch.Size([56, 12, 32, 32])\n",
      "attention torch.Size([56, 12, 32, 32]) EarthSpecificBias torch.Size([1, 56, 12, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([1, 7, 16, 16, 384])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "x torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([56, 32, 384])\n",
      "x_layer torch.Size([1, 7, 16, 16, 384])\n",
      "window torch.Size([56, 32, 384])\n",
      "query torch.Size([56, 12, 32, 32])\n",
      "attention torch.Size([56, 12, 32, 32]) EarthSpecificBias torch.Size([1, 56, 12, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([1, 56, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "x torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([56, 32, 384])\n",
      "x_layer torch.Size([1, 7, 16, 16, 384])\n",
      "window torch.Size([56, 32, 384])\n",
      "query torch.Size([56, 12, 32, 32])\n",
      "attention torch.Size([56, 12, 32, 32]) EarthSpecificBias torch.Size([1, 56, 12, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([1, 7, 16, 16, 384])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "x torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([56, 32, 384])\n",
      "x_layer torch.Size([1, 7, 16, 16, 384])\n",
      "window torch.Size([56, 32, 384])\n",
      "query torch.Size([56, 12, 32, 32])\n",
      "attention torch.Size([56, 12, 32, 32]) EarthSpecificBias torch.Size([1, 56, 12, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([1, 56, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "x torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([56, 32, 384])\n",
      "x_layer torch.Size([1, 7, 16, 16, 384])\n",
      "window torch.Size([56, 32, 384])\n",
      "query torch.Size([56, 12, 32, 32])\n",
      "attention torch.Size([56, 12, 32, 32]) EarthSpecificBias torch.Size([1, 56, 12, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([1, 7, 16, 16, 384])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "x torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([56, 32, 384])\n",
      "x_layer torch.Size([1, 7, 16, 16, 384])\n",
      "window torch.Size([56, 32, 384])\n",
      "query torch.Size([56, 12, 32, 32])\n",
      "attention torch.Size([56, 12, 32, 32]) EarthSpecificBias torch.Size([1, 56, 12, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([1, 56, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "x torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([56, 32, 384])\n",
      "x_layer torch.Size([1, 7, 16, 16, 384])\n",
      "window torch.Size([56, 32, 384])\n",
      "query torch.Size([56, 12, 32, 32])\n",
      "attention torch.Size([56, 12, 32, 32]) EarthSpecificBias torch.Size([1, 56, 12, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([1, 7, 16, 16, 384])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "x torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([56, 32, 384])\n",
      "x_layer torch.Size([1, 7, 16, 16, 384])\n",
      "window torch.Size([56, 32, 384])\n",
      "query torch.Size([56, 12, 32, 32])\n",
      "attention torch.Size([56, 12, 32, 32]) EarthSpecificBias torch.Size([1, 56, 12, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([1, 56, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "x torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([56, 32, 384])\n",
      "x_layer torch.Size([1, 7, 16, 16, 384])\n",
      "window torch.Size([56, 32, 384])\n",
      "query torch.Size([56, 12, 32, 32])\n",
      "attention torch.Size([56, 12, 32, 32]) EarthSpecificBias torch.Size([1, 56, 12, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([1, 7, 16, 16, 384])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "x torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([56, 32, 384])\n",
      "x_layer torch.Size([1, 7, 16, 16, 384])\n",
      "window torch.Size([56, 32, 384])\n",
      "query torch.Size([56, 12, 32, 32])\n",
      "attention torch.Size([56, 12, 32, 32]) EarthSpecificBias torch.Size([1, 56, 12, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([1, 56, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "x torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([56, 32, 384])\n",
      "x_layer torch.Size([1, 7, 32, 32, 192])\n",
      "window torch.Size([224, 32, 192])\n",
      "query torch.Size([224, 6, 32, 32])\n",
      "attention torch.Size([224, 6, 32, 32]) EarthSpecificBias torch.Size([1, 224, 6, 32, 32])\n",
      "attention torch.Size([1, 224, 6, 32, 32])\n",
      "torch.Size([1, 7, 32, 32, 192])\n",
      "attention torch.Size([1, 224, 6, 32, 32])\n",
      "x torch.Size([1, 224, 6, 32, 32])\n",
      "torch.Size([224, 32, 192])\n",
      "x_layer torch.Size([1, 7, 32, 32, 192])\n",
      "window torch.Size([224, 32, 192])\n",
      "query torch.Size([224, 6, 32, 32])\n",
      "attention torch.Size([224, 6, 32, 32]) EarthSpecificBias torch.Size([1, 224, 6, 32, 32])\n",
      "attention torch.Size([1, 224, 6, 32, 32])\n",
      "torch.Size([1, 224, 32, 32])\n",
      "attention torch.Size([1, 224, 6, 32, 32])\n",
      "x torch.Size([1, 224, 6, 32, 32])\n",
      "torch.Size([224, 32, 192])\n",
      "x_layer torch.Size([1, 7, 32, 32, 192])\n",
      "window torch.Size([224, 32, 192])\n",
      "query torch.Size([224, 6, 32, 32])\n",
      "attention torch.Size([224, 6, 32, 32]) EarthSpecificBias torch.Size([1, 224, 6, 32, 32])\n",
      "attention torch.Size([1, 224, 6, 32, 32])\n",
      "torch.Size([1, 7, 32, 32, 192])\n",
      "attention torch.Size([1, 224, 6, 32, 32])\n",
      "x torch.Size([1, 224, 6, 32, 32])\n",
      "torch.Size([224, 32, 192])\n",
      "x_layer torch.Size([1, 7, 32, 32, 192])\n",
      "window torch.Size([224, 32, 192])\n",
      "query torch.Size([224, 6, 32, 32])\n",
      "attention torch.Size([224, 6, 32, 32]) EarthSpecificBias torch.Size([1, 224, 6, 32, 32])\n",
      "attention torch.Size([1, 224, 6, 32, 32])\n",
      "torch.Size([1, 224, 32, 32])\n",
      "attention torch.Size([1, 224, 6, 32, 32])\n",
      "x torch.Size([1, 224, 6, 32, 32])\n",
      "torch.Size([224, 32, 192])\n",
      "layer1start torch.Size([1, 7168, 192])\n",
      "down x torch.Size([1, 7, 32, 32, 192])\n",
      "layer2start torch.Size([1, 1792, 384])\n",
      "x_layer torch.Size([1, 7, 16, 16, 384])\n",
      "window torch.Size([56, 32, 384])\n",
      "query torch.Size([56, 12, 32, 32])\n",
      "attention torch.Size([56, 12, 32, 32]) EarthSpecificBias torch.Size([1, 56, 12, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([1, 7, 16, 16, 384])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "x torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([56, 32, 384])\n",
      "x_layer torch.Size([1, 7, 16, 16, 384])\n",
      "window torch.Size([56, 32, 384])\n",
      "query torch.Size([56, 12, 32, 32])\n",
      "attention torch.Size([56, 12, 32, 32]) EarthSpecificBias torch.Size([1, 56, 12, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([1, 56, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "x torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([56, 32, 384])\n",
      "x_layer torch.Size([1, 7, 16, 16, 384])\n",
      "window torch.Size([56, 32, 384])\n",
      "query torch.Size([56, 12, 32, 32])\n",
      "attention torch.Size([56, 12, 32, 32]) EarthSpecificBias torch.Size([1, 56, 12, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([1, 7, 16, 16, 384])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "x torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([56, 32, 384])\n",
      "x_layer torch.Size([1, 7, 16, 16, 384])\n",
      "window torch.Size([56, 32, 384])\n",
      "query torch.Size([56, 12, 32, 32])\n",
      "attention torch.Size([56, 12, 32, 32]) EarthSpecificBias torch.Size([1, 56, 12, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([1, 56, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "x torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([56, 32, 384])\n",
      "x_layer torch.Size([1, 7, 16, 16, 384])\n",
      "window torch.Size([56, 32, 384])\n",
      "query torch.Size([56, 12, 32, 32])\n",
      "attention torch.Size([56, 12, 32, 32]) EarthSpecificBias torch.Size([1, 56, 12, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([1, 7, 16, 16, 384])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "x torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([56, 32, 384])\n",
      "x_layer torch.Size([1, 7, 16, 16, 384])\n",
      "window torch.Size([56, 32, 384])\n",
      "query torch.Size([56, 12, 32, 32])\n",
      "attention torch.Size([56, 12, 32, 32]) EarthSpecificBias torch.Size([1, 56, 12, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([1, 56, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "x torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([56, 32, 384])\n",
      "x_layer torch.Size([1, 7, 16, 16, 384])\n",
      "window torch.Size([56, 32, 384])\n",
      "query torch.Size([56, 12, 32, 32])\n",
      "attention torch.Size([56, 12, 32, 32]) EarthSpecificBias torch.Size([1, 56, 12, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([1, 7, 16, 16, 384])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "x torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([56, 32, 384])\n",
      "x_layer torch.Size([1, 7, 16, 16, 384])\n",
      "window torch.Size([56, 32, 384])\n",
      "query torch.Size([56, 12, 32, 32])\n",
      "attention torch.Size([56, 12, 32, 32]) EarthSpecificBias torch.Size([1, 56, 12, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([1, 56, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "x torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([56, 32, 384])\n",
      "x_layer torch.Size([1, 7, 16, 16, 384])\n",
      "window torch.Size([56, 32, 384])\n",
      "query torch.Size([56, 12, 32, 32])\n",
      "attention torch.Size([56, 12, 32, 32]) EarthSpecificBias torch.Size([1, 56, 12, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([1, 7, 16, 16, 384])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "x torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([56, 32, 384])\n",
      "x_layer torch.Size([1, 7, 16, 16, 384])\n",
      "window torch.Size([56, 32, 384])\n",
      "query torch.Size([56, 12, 32, 32])\n",
      "attention torch.Size([56, 12, 32, 32]) EarthSpecificBias torch.Size([1, 56, 12, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([1, 56, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "x torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([56, 32, 384])\n",
      "x_layer torch.Size([1, 7, 16, 16, 384])\n",
      "window torch.Size([56, 32, 384])\n",
      "query torch.Size([56, 12, 32, 32])\n",
      "attention torch.Size([56, 12, 32, 32]) EarthSpecificBias torch.Size([1, 56, 12, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([1, 7, 16, 16, 384])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "x torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([56, 32, 384])\n",
      "x_layer torch.Size([1, 7, 16, 16, 384])\n",
      "window torch.Size([56, 32, 384])\n",
      "query torch.Size([56, 12, 32, 32])\n",
      "attention torch.Size([56, 12, 32, 32]) EarthSpecificBias torch.Size([1, 56, 12, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([1, 56, 32, 32])\n",
      "attention torch.Size([1, 56, 12, 32, 32])\n",
      "x torch.Size([1, 56, 12, 32, 32])\n",
      "torch.Size([56, 32, 384])\n",
      "x_layer torch.Size([1, 7, 32, 32, 192])\n",
      "window torch.Size([224, 32, 192])\n",
      "query torch.Size([224, 6, 32, 32])\n",
      "attention torch.Size([224, 6, 32, 32]) EarthSpecificBias torch.Size([1, 224, 6, 32, 32])\n",
      "attention torch.Size([1, 224, 6, 32, 32])\n",
      "torch.Size([1, 7, 32, 32, 192])\n",
      "attention torch.Size([1, 224, 6, 32, 32])\n",
      "x torch.Size([1, 224, 6, 32, 32])\n",
      "torch.Size([224, 32, 192])\n",
      "x_layer torch.Size([1, 7, 32, 32, 192])\n",
      "window torch.Size([224, 32, 192])\n",
      "query torch.Size([224, 6, 32, 32])\n",
      "attention torch.Size([224, 6, 32, 32]) EarthSpecificBias torch.Size([1, 224, 6, 32, 32])\n",
      "attention torch.Size([1, 224, 6, 32, 32])\n",
      "torch.Size([1, 224, 32, 32])\n",
      "attention torch.Size([1, 224, 6, 32, 32])\n",
      "x torch.Size([1, 224, 6, 32, 32])\n",
      "torch.Size([224, 32, 192])\n",
      "torch.Size([1, 5, 12, 128, 128]) torch.Size([1, 4, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "upper = torch.rand(1,5,12,128,128,dtype=torch.float32)\n",
    "surface = torch.rand(1,4,128,128,dtype=torch.float32)\n",
    "\n",
    "model = PanguModel()\n",
    "print(model(upper,surface)[0].shape,model(upper,surface)[1].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
