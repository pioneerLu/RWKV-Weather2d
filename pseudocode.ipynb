{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pseudocode of Pangu-Weather\n",
    "'''\n",
    "# The pseudocode can be implemented using deep learning libraries, e.g., Pytorch and Tensorflow or other high-level APIs\n",
    "\n",
    "# Basic operations used in our model, namely Linear, Conv3d, Conv2d, ConvTranspose3d and ConvTranspose2d\n",
    "# Linear: Linear transformation, available in all deep learning libraries\n",
    "# Conv3d and Con2d: Convolution with 2 or 3 dimensions, available in all deep learning libraries\n",
    "# ConvTranspose3d, ConvTranspose2d: transposed convolution with 2 or 3 dimensions, see Pytorch API or Tensorflow API\n",
    "from Your_AI_Library import Linear, Conv3d, Conv2d, ConvTranspose3d, ConvTranspose2d\n",
    "from torch.nn import Linear, Conv3d, Conv2d, ConvTranspose3d, ConvTranspose2d\n",
    "# Functions in the networks, namely GeLU, DropOut, DropPath, LayerNorm, and SoftMax\n",
    "# GeLU: the GeLU activation function, see Pytorch API or Tensorflow API\n",
    "# DropOut: the dropout function, available in all deep learning libraries\n",
    "# DropPath: the DropPath function, see the implementation of vision-transformer, see timm pakage of Pytorch\n",
    "# A possible implementation of DropPath: from timm.models.layers import DropPath\n",
    "# LayerNorm: the layer normalization function, see Pytorch API or Tensorflow API\n",
    "# Softmax: softmax function, see Pytorch API or Tensorflow API\n",
    "from Your_AI_Library import GeLU, DropOut, DropPath, LayerNorm, SoftMax\n",
    "\n",
    "# Common functions for roll, pad, and crop, depends on the data structure of your software environment\n",
    "from Your_AI_Library import roll3D, pad3D, pad2D, Crop3D, Crop2D\n",
    "\n",
    "# Common functions for reshaping and changing the order of dimensions\n",
    "# reshape: change the shape of the data with the order unchanged, see Pytorch API or Tensorflow API\n",
    "# TransposeDimensions: change the order of the dimensions, see Pytorch API or Tensorflow API\n",
    "from Your_AI_Library import reshape, TransposeDimensions\n",
    "\n",
    "# Common functions for creating new tensors\n",
    "# ConstructTensor: create a new tensor with an arbitrary shape\n",
    "# TruncatedNormalInit: Initialize the tensor with Truncate Normalization distribution\n",
    "# RangeTensor: create a new tensor like range(a, b)\n",
    "from Your_AI_Library import ConstructTensor, TruncatedNormalInit, RangeTensor\n",
    "\n",
    "# Common operations for the data, you may design it or simply use deep learning APIs default operations\n",
    "# LinearSpace: a tensor version of numpy.linspace\n",
    "# MeshGrid: a tensor version of numpy.meshgrid\n",
    "# Stack: a tensor version of numpy.stack\n",
    "# Flatten: a tensor version of numpy.ndarray.flatten\n",
    "# TensorSum: a tensor version of numpy.sum\n",
    "# TensorAbs: a tensor version of numpy.abs\n",
    "# Concatenate: a tensor version of numpy.concatenate\n",
    "from Your_AI_Library import LinearSpace, MeshGrid, Stack, Flatten, TensorSum, TensorAbs, Concatenate\n",
    "\n",
    "# Common functions for training models\n",
    "# LoadModel and SaveModel: Load and save the model, some APIs may require further adaptation to hardwares\n",
    "# Backward: Gradient backward to calculate the gratitude of each parameters\n",
    "# UpdateModelParametersWithAdam: Use Adam to update parameters, e.g., torch.optim.Adam\n",
    "from Your_AI_Library import LoadModel, Backward, UpdateModelParametersWithAdam, SaveModel\n",
    "\n",
    "# Custom functions to read your data from the disc\n",
    "# LoadData: Load the ERA5 data\n",
    "# LoadConstantMask: Load constant masks, e.g., soil type\n",
    "# LoadStatic: Load mean and std of the ERA5 training data, every fields such as T850 is treated as an image and calculate the mean and std\n",
    "from Your_Data_Code import LoadData, LoadConstantMask, LoadStatic\n",
    "\n",
    "\n",
    "def Inference(input, input_surface, forecast_range):\n",
    "  '''Inference code, describing the algorithm of inference using models with different lead times. \n",
    "  PanguModel24, PanguModel6, PanguModel3 and PanguModel1 share the same training algorithm but differ in lead times.\n",
    "  Args:\n",
    "    input: input tensor, need to be normalized to N(0, 1) in practice\n",
    "    input_surface: target tensor, need to be normalized to N(0, 1) in practice\n",
    "    forecast_range: iteration numbers when roll out the forecast model\n",
    "  '''\n",
    "\n",
    "  # Load 4 pre-trained models with different lead times\n",
    "  PanguModel24 = LoadModel(ModelPath24)\n",
    "  PanguModel6 = LoadModel(ModelPath6)\n",
    "  PanguModel3 = LoadModel(ModelPath3)\n",
    "  PanguModel1 = LoadModel(ModelPath1)\n",
    "\n",
    "  # Load mean and std of the weather data\n",
    "  weather_mean, weather_std, weather_surface_mean, weather_surface_std = LoadStatic()\n",
    "\n",
    "  # Store initial input for different models\n",
    "  input_24, input_surface_24 = input, input_surface\n",
    "  input_6, input_surface_6 = input, input_surface\n",
    "  input_3, input_surface_3 = input, input_surface\n",
    "\n",
    "  # Using a list to store output\n",
    "  output_list = []\n",
    "\n",
    "  # Note: the following code is implemented for fast inference of [1,forecast_range]-hour forecasts -- if only one lead time is requested, the inference can be much faster.\n",
    "  for i in range(forecast_range):\n",
    "    # switch to the 24-hour model if the forecast time is 24 hours, 48 hours, ..., 24*N hours\n",
    "    if (i+1) % 24 == 0:\n",
    "      # Switch the input back to the stored input\n",
    "      input, input_surface = input_24, input_surface_24\n",
    "\n",
    "      # Call the model pretrained for 24 hours forecast\n",
    "      output, output_surface = PanguModel24(input, input_surface)\n",
    "\n",
    "      # Restore from uniformed output\n",
    "      output = output * weather_std + weather_mean\n",
    "      output_surface = output_surface * weather_surface_std + weather_surface_mean\n",
    "\n",
    "      # Stored the output for next round forecast\n",
    "      input_24, input_surface_24 = output, output_surface\n",
    "      input_6, input_surface_6 = output, output_surface\n",
    "      input_3, input_surface_3 = output, output_surface\n",
    "\n",
    "    # switch to the 6-hour model if the forecast time is 30 hours, 36 hours, ..., 24*N + 6/12/18 hours\n",
    "    elif (i+1) % 6 == 0:\n",
    "      # Switch the input back to the stored input\n",
    "      input, input_surface = input_6, input_surface_6\n",
    "\n",
    "      # Call the model pretrained for 6 hours forecast\n",
    "      output, output_surface = PanguModel6(input, input_surface)\n",
    "\n",
    "      # Restore from uniformed output\n",
    "      output = output * weather_std + weather_mean\n",
    "      output_surface = output_surface * weather_surface_std + weather_surface_mean\n",
    "      \n",
    "      # Stored the output for next round forecast\n",
    "      input_6, input_surface_6 = output, output_surface\n",
    "      input_3, input_surface_3 = output, output_surface\n",
    "\n",
    "    # switch to the 3-hour model if the forecast time is 3 hours, 9 hours, ..., 6*N + 3 hours\n",
    "    elif (i+1) % 3 ==0:\n",
    "      # Switch the input back to the stored input\n",
    "      input, input_surface = input_3, input_surface_3\n",
    "\n",
    "      # Call the model pretrained for 3 hours forecast\n",
    "      output, output_surface = PanguModel3(input, input_surface)\n",
    "\n",
    "      # Restore from uniformed output\n",
    "      output = output * weather_std + weather_mean\n",
    "      output_surface = output_surface * weather_surface_std + weather_surface_mean\n",
    "      \n",
    "      # Stored the output for next round forecast\n",
    "      input_3, input_surface_3 = output, output_surface\n",
    "\n",
    "    # switch to the 1-hour model\n",
    "    else:\n",
    "      # Call the model pretrained for 1 hours forecast\n",
    "      output, output_surface = PanguModel1(input, input_surface)\n",
    "\n",
    "      # Restore from uniformed output\n",
    "      output = output * weather_std + weather_mean\n",
    "      output_surface = output_surface * weather_surface_std + weather_surface_mean\n",
    "\n",
    "    # Stored the output for next round forecast\n",
    "    input, input_surface = output, output_surface\n",
    "\n",
    "    # Save the output\n",
    "    output_list.append((output, output_surface))\n",
    "  return output_list\n",
    "\n",
    "\n",
    "def Train():\n",
    "  '''Training code'''\n",
    "  # Initialize the model, for some APIs some adaptation is needed to fit hardwares\n",
    "  model = PanguModel()\n",
    "\n",
    "  # Train single Pangu-Weather model\n",
    "  epochs = 100\n",
    "  for i in range(epochs):\n",
    "    # For each epoch, we iterate from 1979 to 2017\n",
    "    # dataset_length is the length of your training data, e.g., the sample between 1979 and 2017\n",
    "    for step in range(dataset_length):\n",
    "      # Load weather data at time t as the input; load weather data at time t+1/3/6/24 as the output\n",
    "      # Note the data need to be randomly shuffled\n",
    "      # Note the input and target need to be normalized, see Inference() for details\n",
    "      input, input_surface, target, target_surface = LoadData(step)\n",
    "\n",
    "      # Call the model and get the output\n",
    "      output, output_surface = model(input, input_surface)\n",
    "\n",
    "      # We use the MAE loss to train the model\n",
    "      # The weight of surface loss is 0.25\n",
    "      # Different weight can be applied for differen fields if needed\n",
    "      loss = TensorAbs(output-target) + TensorAbs(output_surface-target_surface) * 0.25\n",
    "\n",
    "      # Call the backward algorithm and calculate the gratitude of parameters\n",
    "      Backward(loss)\n",
    "\n",
    "      # Update model parameters with Adam optimizer\n",
    "      # The learning rate is 5e-4 as in the paper, while the weight decay is 3e-6\n",
    "      # A example solution is using torch.optim.adam\n",
    "      UpdateModelParametersWithAdam()\n",
    "\n",
    "  # Save the model at the end of the training stage\n",
    "  SaveModel()\n",
    "\n",
    "class PanguModel:\n",
    "  def __init__(self):\n",
    "    # Drop path rate is linearly increased as the depth increases\n",
    "    drop_path_list = LinearSpace(0, 0.2, 8)\n",
    "\n",
    "    # Patch embedding\n",
    "    self._input_layer = PatchEmbedding((2, 4, 4), 192)\n",
    "\n",
    "    # Four basic layers\n",
    "    self.layer1 = EarthSpecificLayer(2, 192, drop_list[:2], 6)\n",
    "    self.layer2 = EarthSpecificLayer(6, 384, drop_list[6:], 12)\n",
    "    self.layer3 = EarthSpecificLayer(6, 384, drop_list[6:], 12)\n",
    "    self.layer4 = EarthSpecificLayer(2, 192, drop_list[:2], 6)\n",
    "\n",
    "    # Upsample and downsample\n",
    "    self.upsample = UpSample(384, 192)\n",
    "    self.downsample = DownSample(192)\n",
    "\n",
    "    # Patch Recovery\n",
    "    self._output_layer = PatchRecovery(384)\n",
    "    \n",
    "  def forward(self, input, input_surface):\n",
    "    '''Backbone architecture'''\n",
    "    # Embed the input fields into patches\n",
    "    x = self._input_layer(input, input_surface)\n",
    "\n",
    "    # Encoder, composed of two layers\n",
    "    # Layer 1, shape (8, 360, 181, C), C = 192 as in the original paper\n",
    "    x = self.layer1(x, 8, 360, 181) \n",
    "\n",
    "    # Store the tensor for skip-connection\n",
    "    skip = x\n",
    "\n",
    "    # Downsample from (8, 360, 181) to (8, 180, 91)\n",
    "    x = self.downsample(x, 8, 360, 181)\n",
    "\n",
    "    # Layer 2, shape (8, 180, 91, 2C), C = 192 as in the original paper\n",
    "    x = self.layer2(x, 8, 180, 91) \n",
    "\n",
    "    # Decoder, composed of two layers\n",
    "    # Layer 3, shape (8, 180, 91, 2C), C = 192 as in the original paper\n",
    "    x = self.layer3(x, 8, 180, 91) \n",
    "\n",
    "    # Upsample from (8, 180, 91) to (8, 360, 181)\n",
    "    x = self.upsample(x)\n",
    "\n",
    "    # Layer 4, shape (8, 360, 181, 2C), C = 192 as in the original paper\n",
    "    x = self.layer4(x, 8, 360, 181) \n",
    "\n",
    "    # Skip connect, in last dimension(C from 192 to 384)\n",
    "    x = Concatenate(skip, x)\n",
    "\n",
    "    # Recover the output fields from patches\n",
    "    output, output_surface = self._output_layer(x)\n",
    "    return output, output_surface\n",
    "\n",
    "class PatchEmbedding:\n",
    "  def __init__(self, patch_size, dim):\n",
    "    '''Patch embedding operation'''\n",
    "    # Here we use convolution to partition data into cubes\n",
    "    self.conv = Conv3d(input_dims=5, output_dims=dim, kernel_size=patch_size, stride=patch_size)\n",
    "    self.conv_surface = Conv2d(input_dims=7, output_dims=dim, kernel_size=patch_size[1:], stride=patch_size[1:])\n",
    "\n",
    "    # Load constant masks from the disc\n",
    "    self.land_mask, self.soil_type, self.topography = LoadConstantMask()\n",
    "    \n",
    "  def forward(self, input, input_surface):\n",
    "    # Zero-pad the input\n",
    "    input = Pad3D(input)\n",
    "    input_surface = Pad2D(input_surface)\n",
    "\n",
    "    # Apply a linear projection for patch_size[0]*patch_size[1]*patch_size[2] patches, patch_size = (2, 4, 4) as in the original paper\n",
    "    input = self.conv(input)\n",
    "\n",
    "    # Add three constant fields to the surface fields\n",
    "    input_surface =  Concatenate(input_surface, self.land_mask, self.soil_type, self.topography)\n",
    "\n",
    "    # Apply a linear projection for patch_size[1]*patch_size[2] patches\n",
    "    input_surface = self.conv_surface(input_surface)\n",
    "\n",
    "    # Concatenate the input in the pressure level, i.e., in Z dimension\n",
    "    x = Concatenate(input, input_surface)\n",
    "\n",
    "    # Reshape x for calculation of linear projections\n",
    "    x = TransposeDimensions(x, (0, 2, 3, 4, 1))\n",
    "    x = reshape(x, target_shape=(x.shape[0], 8*360*181, x.shape[-1]))\n",
    "    return x\n",
    "    \n",
    "class PatchRecovery:\n",
    "  def __init__(self, dim):\n",
    "    '''Patch recovery operation'''\n",
    "    # Hear we use two transposed convolutions to recover data\n",
    "    self.conv = ConvTranspose3d(input_dims=dim, output_dims=5, kernel_size=patch_size, stride=patch_size)\n",
    "    self.conv_surface = ConvTranspose2d(input_dims=dim, output_dims=4, kernel_size=patch_size[1:], stride=patch_size[1:])\n",
    "    \n",
    "  def forward(self, x, Z, H, W):\n",
    "    # The inverse operation of the patch embedding operation, patch_size = (2, 4, 4) as in the original paper\n",
    "    # Reshape x back to three dimensions\n",
    "    x = TransposeDimensions(x, (0, 2, 1))\n",
    "    x = reshape(x, target_shape=(x.shape[0], x.shape[1], Z, H, W))\n",
    "\n",
    "    # Call the transposed convolution\n",
    "    output = self.conv(x[:, :, 1:, :, :])\n",
    "    output_surface = self.conv_surface(x[:, :, 0, :, :])\n",
    "\n",
    "    # Crop the output to remove zero-paddings\n",
    "    output = Crop3D(output)\n",
    "    output_surface = Crop2D(output_surface)\n",
    "    return output, output_surface\n",
    "\n",
    "class DownSample:\n",
    "  def __init__(self, dim):\n",
    "    '''Down-sampling operation'''\n",
    "    # A linear function and a layer normalization\n",
    "    self.linear = Linear(4*dim, 2*dim, bias=Fasle)\n",
    "    self.norm = LayerNorm(4*dim)\n",
    "  \n",
    "  def forward(self, x, Z, H, W):\n",
    "    # Reshape x to three dimensions for downsampling\n",
    "    x = reshape(x, target_shape=(x.shape[0], Z, H, W, x.shape[-1]))\n",
    "\n",
    "    # Padding the input to facilitate downsampling\n",
    "    x = Pad3D(x)\n",
    "\n",
    "    # Reorganize x to reduce the resolution: simply change the order and downsample from (8, 360, 182) to (8, 180, 91)\n",
    "    Z, H, W = x.shape\n",
    "    # Reshape x to facilitate downsampling\n",
    "    x = reshape(x, target_shape=(x.shape[0], Z, H//2, 2, W//2, 2, x.shape[-1]))\n",
    "    # Change the order of x\n",
    "    x = TransposeDimensions(x, (0,1,2,4,3,5,6))\n",
    "    # Reshape to get a tensor of resolution (8, 180, 91)\n",
    "    x = reshape(x, target_shape=(x.shape[0], Z*(H//2)*(W//2), 4 * x.shape[-1]))\n",
    "\n",
    "    # Call the layer normalization\n",
    "    x = self.norm(x)\n",
    "\n",
    "    # Decrease the channels of the data to reduce computation cost\n",
    "    x = self.linear(x)\n",
    "    return x\n",
    "\n",
    "class UpSample:\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "    '''Up-sampling operation'''\n",
    "    # Linear layers without bias to increase channels of the data\n",
    "    self.linear1 = Linear(input_dim, output_dim*4, bias=False)\n",
    "\n",
    "    # Linear layers without bias to mix the data up\n",
    "    self.linear2 = Linear(output_dim, output_dim, bias=False)\n",
    "\n",
    "    # Normalization\n",
    "    self.norm = LayerNorm(output_dim)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    # Call the linear functions to increase channels of the data\n",
    "    x = self.linear1(x)\n",
    "\n",
    "    # Reorganize x to increase the resolution: simply change the order and upsample from (8, 180, 91) to (8, 360, 182)\n",
    "    # Reshape x to facilitate upsampling.\n",
    "    x = reshape(x, target_shape=(x.shape[0], 8, 180, 91, 2, 2, x.shape[-1]//4))\n",
    "    # Change the order of x\n",
    "    x = TransposeDimensions(x, (0,1,2,4,3,5,6))\n",
    "    # Reshape to get Tensor with a resolution of (8, 360, 182)\n",
    "    x = reshape(x, target_shape=(x.shape[0], 8, 360, 182, x.shape[-1]))\n",
    "\n",
    "    # Crop the output to the input shape of the network\n",
    "    x = Crop3D(x)\n",
    "\n",
    "    # Reshape x back\n",
    "    x = reshape(x, target_shape=(x.shape[0], x.shape[1]*x.shape[2]*x.shape[3], x.shape[-1]))\n",
    "\n",
    "    # Call the layer normalization\n",
    "    x = self.norm(x)\n",
    "\n",
    "    # Mixup normalized tensors\n",
    "    x = self.linear2(x)\n",
    "    return x\n",
    "  \n",
    "class EarthSpecificLayer:\n",
    "  def __init__(self, depth, dim, drop_path_ratio_list, heads):\n",
    "    '''Basic layer of our network, contains 2 or 6 blocks'''\n",
    "    self.depth = depth\n",
    "    self.blocks = []\n",
    "\n",
    "    # Construct basic blocks\n",
    "    for i in range(depth):\n",
    "      self.blocks.append(EarthSpecificBlock(dim, drop_path_ratio_list[i], heads))\n",
    "      \n",
    "  def forward(self, x, Z, H, W):\n",
    "    for i in range(self.depth):\n",
    "      # Roll the input every two blocks\n",
    "      if i % 2 == 0:\n",
    "        self.blocks[i](x, Z, H, W, roll=False)\n",
    "      else:\n",
    "        self.blocks[i](x, Z, H, W, roll=True)\n",
    "    return x\n",
    "\n",
    "class EarthSpecificBlock:\n",
    "  def __init__(self, dim, drop_path_ratio, heads):\n",
    "    '''\n",
    "    3D transformer block with Earth-Specific bias and window attention, \n",
    "    see https://github.com/microsoft/Swin-Transformer for the official implementation of 2D window attention.\n",
    "    The major difference is that we expand the dimensions to 3 and replace the relative position bias with Earth-Specific bias.\n",
    "    '''\n",
    "    # Define the window size of the neural network \n",
    "    self.window_size = (2, 6, 12)\n",
    "\n",
    "    # Initialize serveral operations\n",
    "    self.drop_path = DropPath(drop_rate=drop_path_ratio)\n",
    "    self.norm1 = LayerNorm(dim)\n",
    "    self.norm2 = LayerNorm(dim)\n",
    "    self.linear = MLP(dim, 0)\n",
    "    self.attention = EarthAttention3D(dim, heads, 0, self.window_size)\n",
    "\n",
    "  def forward(self, x, Z, H, W, roll):\n",
    "    # Save the shortcut for skip-connection\n",
    "    shortcut = x\n",
    "\n",
    "    # Reshape input to three dimensions to calculate window attention\n",
    "    reshape(x, target_shape=(x.shape[0], Z, H, W, x.shape[2]))\n",
    "\n",
    "    # Zero-pad input if needed\n",
    "    x = pad3D(x)\n",
    "\n",
    "    # Store the shape of the input for restoration\n",
    "    ori_shape = x.shape\n",
    "\n",
    "    if roll:\n",
    "      # Roll x for half of the window for 3 dimensions\n",
    "      x = roll3D(x, shift=[self.window_size[0]//2, self.window_size[1]//2, self.window_size[2]//2])\n",
    "      # Generate mask of attention masks\n",
    "      # If two pixels are not adjacent, then mask the attention between them\n",
    "      # Your can set the matrix element to -1000 when it is not adjacent, then add it to the attention\n",
    "      mask = gen_mask(x)\n",
    "    else:\n",
    "      # e.g., zero matrix when you add mask to attention\n",
    "      mask = no_mask\n",
    "\n",
    "    # Reorganize data to calculate window attention\n",
    "    x_window = reshape(x, target_shape=(x.shape[0], Z//window_size[0], window_size[0], H // window_size[1], window_size[1], W // window_size[2], window_size[2], x.shape[-1]))\n",
    "    x_window = TransposeDimensions(x_window, (0, 1, 3, 5, 2, 4, 6, 7))\n",
    "\n",
    "    # Get data stacked in 3D cubes, which will further be used to calculated attention among each cube\n",
    "    x_window = reshape(x_window, target_shape=(-1, window_size[0]* window_size[1]*window_size[2], x.shape[-1]))\n",
    "\n",
    "    # Apply 3D window attention with Earth-Specific bias\n",
    "    x_window = self.attention(x, mask)\n",
    "\n",
    "    # Reorganize data to original shapes\n",
    "    x = reshape(x_window, target_shape=((-1, Z // window_size[0], H // window_size[1], W // window_size[2], window_size[0], window_size[1], window_size[2], x_window.shape[-1])))\n",
    "    x = TransposeDimensions(x, (0, 1, 4, 2, 5, 3, 6, 7))\n",
    "\n",
    "    # Reshape the tensor back to its original shape\n",
    "    x = reshape(x_window, target_shape=ori_shape)\n",
    "\n",
    "    if roll:\n",
    "      # Roll x back for half of the window\n",
    "      x = roll3D(x, shift=[-self.window_size[0]//2, -self.window_size[1]//2, -self.window_size[2]//2])\n",
    "\n",
    "    # Crop the zero-padding\n",
    "    x = Crop3D(x)\n",
    "\n",
    "    # Reshape the tensor back to the input shape\n",
    "    x = reshape(x, target_shape=(x.shape[0], x.shape[1]*x.shape[2]*x.shape[3], x.shape[4]))\n",
    "\n",
    "    # Main calculation stages\n",
    "    x = shortcut + self.drop_path(self.norm1(x))\n",
    "    x = x + self.drop_path(self.norm2(self.linear(x)))\n",
    "    return x\n",
    "    \n",
    "class EarthAttention3D:\n",
    "  def __init__(self, dim, heads, dropout_rate, window_size):\n",
    "    '''\n",
    "    3D window attention with the Earth-Specific bias, \n",
    "    see https://github.com/microsoft/Swin-Transformer for the official implementation of 2D window attention.\n",
    "    '''\n",
    "    # Initialize several operations\n",
    "    self.linear1 = Linear(dim, dim=3, bias=True)\n",
    "    self.linear2 = Linear(dim, dim)\n",
    "    self.softmax = SoftMax(dim=-1)\n",
    "    self.dropout = DropOut(dropout_rate)\n",
    "\n",
    "    # Store several attributes\n",
    "    self.head_number = heads\n",
    "    self.dim = dim\n",
    "    self.scale = (dim//heads)**-0.5\n",
    "    self.window_size = window_size\n",
    "\n",
    "    # input_shape is current shape of the self.forward function\n",
    "    # You can run your code to record it, modify the code and rerun it\n",
    "    # Record the number of different window types\n",
    "    self.type_of_windows = (input_shape[0]//window_size[0])*(input_shape[1]//window_size[1])\n",
    "\n",
    "    # For each type of window, we will construct a set of parameters according to the paper\n",
    "    self.earth_specific_bias = ConstructTensor(shape=((2 * window_size[2] - 1) * window_size[1] * window_size[1] * window_size[0] * window_size[0], self.type_of_windows, heads))\n",
    "\n",
    "    # Making these tensors to be learnable parameters\n",
    "    self.earth_specific_bias = Parameters(self.earth_specific_bias)\n",
    "\n",
    "    # Initialize the tensors using Truncated normal distribution\n",
    "    TruncatedNormalInit(self.earth_specific_bias, std=0.02) \n",
    "\n",
    "    # Construct position index to reuse self.earth_specific_bias\n",
    "    self.position_index = self._construct_index()\n",
    "    \n",
    "  def _construct_index(self):\n",
    "    ''' This function construct the position index to reuse symmetrical parameters of the position bias'''\n",
    "    # Index in the pressure level of query matrix\n",
    "    coords_zi = RangeTensor(self.window_size[0])\n",
    "    # Index in the pressure level of key matrix\n",
    "    coords_zj = -RangeTensor(self.window_size[0])*self.window_size[0]\n",
    "\n",
    "    # Index in the latitude of query matrix\n",
    "    coords_hi = RangeTensor(self.window_size[1])\n",
    "    # Index in the latitude of key matrix\n",
    "    coords_hj = -RangeTensor(self.window_size[1])*self.window_size[1]\n",
    "\n",
    "    # Index in the longitude of the key-value pair\n",
    "    coords_w = RangeTensor(self.window_size[2])\n",
    "\n",
    "    # Change the order of the index to calculate the index in total\n",
    "    coords_1 = Stack(MeshGrid([coords_zi, coords_hi, coords_w]))\n",
    "    coords_2 = Stack(MeshGrid([coords_zj, coords_hj, coords_w]))\n",
    "    coords_flatten_1 = Flatten(coords_1, start_dimension=1) \n",
    "    coords_flatten_2 = Flatten(coords_2, start_dimension=1)\n",
    "    coords = coords_flatten_1[:, :, None] - coords_flatten_2[:, None, :]\n",
    "    coords = TransposeDimensions(coords, (1, 2, 0))\n",
    "\n",
    "    # Shift the index for each dimension to start from 0\n",
    "    coords[:, :, 2] += self.window_size[2] - 1\n",
    "    coords[:, :, 1] *= 2 * self.window_size[2] - 1\n",
    "    coords[:, :, 0] *= (2 * self.window_size[2] - 1)*self.window_size[1]*self.window_size[1]\n",
    "\n",
    "    # Sum up the indexes in three dimensions\n",
    "    self.position_index = TensorSum(coords, dim=-1)\n",
    "\n",
    "    # Flatten the position index to facilitate further indexing\n",
    "    self.position_index = Flatten(self.position_index)\n",
    "    \n",
    "  def forward(self, x, mask):\n",
    "    # Linear layer to create query, key and value\n",
    "    x = self.linear1(x)\n",
    "\n",
    "    # Record the original shape of the input\n",
    "    original_shape = x.shape\n",
    "\n",
    "    # reshape the data to calculate multi-head attention\n",
    "    qkv = reshape(x, target_shape=(x.shape[0], x.shape[1], 3, self.head_number, self.dim // self.head_number)) \n",
    "    query, key, value = TransposeDimensions(qkv, (2, 0, 3, 1, 4))\n",
    "\n",
    "    # Scale the attention\n",
    "    query = query * self.scale\n",
    "\n",
    "    # Calculated the attention, a learnable bias is added to fix the nonuniformity of the grid.\n",
    "    attention = query @ key.T # @ denotes matrix multiplication\n",
    "\n",
    "    # self.earth_specific_bias is a set of neural network parameters to optimize. \n",
    "    EarthSpecificBias = self.earth_specific_bias[self.position_index]\n",
    "\n",
    "    # Reshape the learnable bias to the same shape as the attention matrix\n",
    "    EarthSpecificBias = reshape(EarthSpecificBias, target_shape=(self.window_size[0]*self.window_size[1]*self.window_size[2], self.window_size[0]*self.window_size[1]*self.window_size[2], self.type_of_windows, self.head_number))\n",
    "    EarthSpecificBias = TransposeDimensions(EarthSpecificBias, (2, 3, 0, 1))\n",
    "    EarthSpecificBias = reshape(EarthSpecificBias, target_shape = [1]+EarthSpecificBias.shape)\n",
    "\n",
    "    # Add the Earth-Specific bias to the attention matrix\n",
    "    attention = attention + EarthSpecificBias\n",
    "\n",
    "    # Mask the attention between non-adjacent pixels, e.g., simply add -100 to the masked element.\n",
    "    attention = self.mask_attention(attention, mask)\n",
    "    attention = self.softmax(attention)\n",
    "    attention = self.dropout(attention)\n",
    "\n",
    "    # Calculated the tensor after spatial mixing.\n",
    "    x = attention @ value.T # @ denote matrix multiplication\n",
    "\n",
    "    # Reshape tensor to the original shape\n",
    "    x = TransposeDimensions(x, (0, 2, 1))\n",
    "    x = reshape(x, target_shape = original_shape)\n",
    "\n",
    "    # Linear layer to post-process operated tensor\n",
    "    x = self.linear2(x)\n",
    "    x = self.dropout(x)\n",
    "    return x\n",
    "  \n",
    "class Mlp:\n",
    "  def __init__(self, dim, dropout_rate):\n",
    "    '''MLP layers, same as most vision transformer architectures.'''\n",
    "    self.linear1 = Linear(dim, dim * 4)\n",
    "    self.linear2 = Linear(dim * 4, dim)\n",
    "    self.activation = GeLU()\n",
    "    self.drop = DropOut(drop_rate=dropout_rate)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = self.linear(x)\n",
    "    x = self.activation(x)\n",
    "    x = self.drop(x)\n",
    "    x = self.linear(x)\n",
    "    x = self.drop(x)\n",
    "    return x\n",
    "\n",
    "def PerlinNoise():\n",
    "  '''Generate random Perlin noise: we follow https://github.com/pvigier/perlin-numpy/ to calculate the perlin noise.'''\n",
    "  # Define number of noise\n",
    "  octaves = 3\n",
    "  # Define the scaling factor of noise\n",
    "  noise_scale = 0.2\n",
    "  # Define the number of periods of noise along the axis\n",
    "  period_number = 12\n",
    "  # The size of an input slice\n",
    "  H, W = 721, 1440\n",
    "  # Scaling factor between two octaves\n",
    "  persistence = 0.5\n",
    "  # see https://github.com/pvigier/perlin-numpy/ for the implementation of GenerateFractalNoise (e.g., from perlin_numpy import generate_fractal_noise_3d)\n",
    "  perlin_noise = noise_scale*GenerateFractalNoise((H, W), (period_number, period_number), octaves, persistence)\n",
    "  return perlin_noise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
